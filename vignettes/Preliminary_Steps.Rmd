---
title: "Preliminary Steps"
author: "Isabella Stallworthy"
date: "`r Sys.Date()`"
output:
    html_vignette:
        
        df_print: kable
        toc: false
vignette: >
  %\VignetteIndexEntry{Preliminary_Steps}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown_notangle}
editor_options:
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

options(rmarkdown.html_vignette.check_title = FALSE)
```
  
<br>
  
These four recommended preliminary steps are designed to assist the user in preparing and inspecting their data to ensure appropriate use of the package. Users should first view the <a href="https://istallworthy.github.io/devMSMs/articles/Terminology.html">Terminology</a> vignette and complete the <a href="https://istallworthy.github.io/devMSMs/articles/Data_Requirements.html">Data Requirements</a> and <a href="https://istallworthy.github.io/devMSMs/articles/Specify_Core_Inputs.html">Specifying Core Inputs</a> vignettes.   
  
The helper functions (summarized in Table 1) for the following recommended preliminary steps can be found at this <a href="https://github.com/istallworthy/devMSMsHelpers">Github</a>. Of note, *devMSMs* must also be installed and loaded to use these helper functions (see <a href="https://istallworthy.github.io/devMSMs/">Installation</a>).  
  
The user who has already formatted their data in wide format according to the Data Requirements vignette and imputed to accommodate any missing data (P2), can focus only on subsections P3 Identifying Optional Exposure Epochs and P4 Verifying History Distributions prior to using the package. Following completion of this vignette, users should use one of the Workflows vignettes to implement *devMSMs* with their longitudinal data.   


*insert Table 1 here*.   

<br>

```{r setup}
# install.packages("devtools")
# install.packages("stats")

require(devtools)
# library(stats)

devtools::install_github("istallworthy/devMSMs", quiet = TRUE)
library(devMSMs)

devtools::install_github("istallworthy/devMSMsHelpers", quiet = TRUE)
library(devMSMsHelpers)
```
<br>

Choose from the following preliminary steps with the goal of assigning to 'data' one of the following wide data formats for use in the package:  
  
* a single data frame of data in wide format with no missing data  
  
* a mids object (output from mice::mice()) of data imputed in wide format  
  
* a list of data imputed in wide format as data frames.  
  
Data columns should be either numeric, integer, or factor form and the ID column should be numeric.  

Some helper functions have optional arguments to suppress saving output locally (`save.out = FALSE`) and printing it to the console ( `verbose = FALSE`). The defaults to both arguments are TRUE. Users must supply a path to a home directory if `save.out = TRUE`. 

```{r}
save.out = FALSE

verbose = TRUE
```
<br>


### Core inputs
Please see the <a href="https://istallworthy.github.io/devMSMs/articles/Specify_Core_Inputs.html">Specifying Core Inputs vignette</a> for more detail on the following core inputs. Here, we use ESETA1, a measure of economic strain experienced by the family, as the exposure and StrDif_Tot, or behavior problems measured by the SDQ, as the outcome.  

```{r}
set.seed(1234)

home_dir <- NA
# home_dir <- '/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/isa' #note: no / after

exposure <- "ESETA1"

exposure_time_pts <- c(6, 15, 24, 35, 58)

outcome <- "StrDif_Tot.58"

tv_confounders <- c("SAAmylase.6","SAAmylase.15", "SAAmylase.24",
                    "MDI.6", "MDI.15",                                            
                    "RHasSO.6", "RHasSO.15", "RHasSO.24","RHasSO.35", "RHasSO.58",                                         
                    "WndNbrhood.6","WndNbrhood.24", "WndNbrhood.35", "WndNbrhood.58",                                       
                    "IBRAttn.6", "IBRAttn.15", "IBRAttn.24",                                   
                    "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58",                                           
                    "HOMEETA1.6", "HOMEETA1.15", "HOMEETA1.24", "HOMEETA1.35", "HOMEETA1.58",                               
                    "InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58",                         
                    "ESETA1.6", "ESETA1.15", "ESETA1.24", "ESETA1.35", "ESETA1.58",               
                    "CORTB.6", "CORTB.15", "CORTB.24",                                                                  
                    "EARS_TJo.24", "EARS_TJo.35",                                        
                    "LESMnPos.24", "LESMnPos.35",                                  
                    "LESMnNeg.24", "LESMnNeg.35",       
                    "StrDif_Tot.35", "StrDif_Tot.58",    
                    "fscore.35", "fscore.58"
) 

#required
ti_confounders <- c("state", "BioDadInHH2", "PmAge2", "PmBlac2", "TcBlac2", "PmMrSt2", "PmEd2", "KFASTScr",
                    "RMomAgeU", "RHealth", "HomeOwnd", "SWghtLB", "SurpPreg", "SmokTotl", "DrnkFreq",
                    "peri_health", "caregiv_health", "gov_assist"
                    #, "state:SmokTotl", "PmAge2:PmBlac2", "PmAge2:PmEd2" #testing interaction terms
)
```
<br>
<br>

### Load data 
Users have several options for reading in data. They can begin this workflow with the following options:  
  
* long data (with or without missing data), which can be converted to wide data
  
* wide data (with or without missing data), which can be imputed using the *imputeData()* helper function, if needed 
  
* imputed wide data sets saved locally, which can be read in as a list

Below, we demonstrate use of each of the above starting options.  
<br>
  
First we load the simulated longitudinal data in long format (with missingness) that accompanies *devMSMs*. These data are simulated based on data from the Family Life Project (FLP), a longitudinal study following 1,292 families representative of two geographic areas (three counties in North Carolina and three counties in Pennsylvania) with high rural child poverty (Vernon-Feagans et al., 2013; Burchinal et al., 2008).  

```{r}
data("sim_data_long_miss", package = "devMSMs")

data_long <- sim_data_long_miss
```
<br>

### P1. Format Data  
All data must be wide format and contain an “ID” column for subject identifier and exposure, outcome, and all confounders as separate columns (as shown in Figure 1). Column names can include only underscore special characters and time-varying variables should have a suffix that consists of a period followed by the time point (e.g., “variable.6”). All variables should be classed as integer, numeric, or a factor (not character). Auxiliary or nuisance covariates that are not confounders (e.g, assessment version) can be included in the dataset for use and specification in the final modeling step (*Workflow* vignettes Step 5).   
<br>

*insert Figure 1 here*. 

<br>

#### P1a. Format single data frame of long data
Users beginning with a single data frame in long format (with or without missingness) can utilize a helper function `formatLongData()` to summarize exposure and outcome data and convert to required variable names. This function takes a dataset in long format and any variables for time (`time_var`), ID (`id_var`), and missing data (missing) with alternative variables and re-labels them according to what is required by the package. It also classes any factor confounders (`factor_confounders`) as factors in the data and all others as numeric.   

```{r}
data_long_f <- formatLongData(data = data_long, exposure = exposure, 
                              exposure_time_pts = exposure_time_pts, outcome = outcome, 
                              time_var = "WAVE", 
                              id_var = "ID",                                
                              missing = NA,
                              factor_confounders = c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                                                     "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                                                     "RHasSO"),                              
                              home_dir = home_dir, 
                              save.out = save.out) 
```
We get a descriptive statistics summary of the exposure, ESETA1, and the outcome, StrDif_Tot.58, for our visual inspections.  

<br>

#### P1b. Convert single long data frame to wide format
Users with correctly formatted variables in long format have the option of using the following code to transform their data into wide format, to proceed to using the package (if there is no missing data) or imputing (with < 20% missing data MAR).  
  
We then transform our newly formatted long data into wide format.  

```{r}
require("stats")
v <- sapply(strsplit(tv_confounders[!grepl("\\:", tv_confounders)], "\\."), "[", 1)
v <- v[!duplicated(v)]

data_wide <- stats::reshape(data = data_long_f, 
                            idvar = "ID", #list ID variable in your dataset
                            v.names = v, 
                            timevar = "WAVE", # list time variable in your long dataset
                            times = c(6, 15, 24, 35, 58), # list all time points in your dataset
                            direction = "wide")

data_wide <- data_wide[, colSums(is.na(data_wide)) < nrow(data_wide)]
```
  
Alternatively, we could start with wide data with missingness that is already formatted. 

```{r}
data("sim_data_wide_miss", package = "devMSMs")

data_wide <- sim_data_wide_miss
```

<br>


### P2. Impute Data to Account for Missingness
The functions of the *devMSMs* package accept data in the form of a single data frame with no missing values or m imputed datasets in the form of either a mids object (output from the mice package or via `imputeData()`) or a list of imputed datasets. Most developmental data from humans will have some amount of missing data. Given that the creation of IPTW balancing weights requires complete data, we recommend imputing data. Imputation assumes a missing data mechanism of missing at random (MAR) and no more than 20% missing data in total (Leyrat et al., 2021). Given existing work demonstrating its superiority, *devMSMS* implements the ‘within’ approach for imputed data, conducting all steps on each imputed dataset before pooling estimates using Rubin’s rules to create final average predictions and contrast comparisons in *Worfklows* vignettes Step 5 (Leyrat et al, 2021; Granger et al., 2019).  

<br>

#### P2a. Multiply impute single wide, formatted data frame using mice
Users have the option of using the helper `imputeData()` function to impute their correctly formatted wide data. This step can take a while to run. The user can specify how many imputed datasets to create (default m = 5). `imputeData()` draws on the `mice()` function from the *mice* package (van Buuren & Oudshoorn, 2011) to conduct multiple imputation by chained equations (mice). All other variables present in the dataset are used to impute missing data in each column.  

The user can specify the imputation method through the `method` field drawing from the following list: “pmm” (predictive mean matching), “midastouch” (weighted predictive mean matching), “sample” (random sample from observed values), “rf” (random forest) or “cart” (classification and regression trees). Random forest imputation is the default given evidence for its efficiency and superior performance (Shah et al., 2014). Please review the *mice* documentation for more details.  

Additionally, users can specify an integer value to `seed` in order to offset the random number generator in *mice()* and make reproducible imputations.  
  
The parameter `read_imps_from_file` will allow you to read already imputed data in from local storage (TRUE) so as not to have to re-run this imputation code multiple times (FALSE; default). Users may use this parameter to supply their own mids object of imputed data from the *mice* package  (with the title ‘all_imp.rds’). Be sure to inspect the console for any warnings as well as the resulting imputed datasets. Any variables that have missing data following imputation may need to be removed due to high collinearity and/or low variability.   

The required inputs for this function are a data frame in wide format (formatted according to pre-requirements listed above), m number of imputed datasets to create, a path to the home directory (if `save.out = TRUE`), exposure (e.g., “variable”), and outcome (e.g., “variable.t”). The home directory path, exposure, and outcome should already be defined if the user completed the <a href="https://istallworthy.github.io/devMSMs/articles/Specify_Core_Inputs.html">Specifying Core Inputs vignette</a>.

The optional inputs are as follows.  
  
The user can specify an imputation method compatible with `mice()` (see above). Additionally, the user can specify in `maxit` the number of interactions for `mice::mice()` to conduct (default is 5). The user can also specify `para_proc`, a logical indicator indicating whether or not to speed up imputing using parallel processing (default = TRUE). This draws on 2 cores using functions from the *parallel*, *doRNG*, and *doParallel* packages.   

The user may also specify any additional inputs accepted by `mice::mice()` and we advise consulting the  <a href:="https://www.rdocumentation.org/packages/mice/versions/3.16.0/topics/mice">[*mice* documentation]</a> for more information.     
The user can also indicate if they have already created imputed datasets from this function and wish to read them in (`read_imps_from_file = TRUE` rather than recreate them (default).  

For this example, we create 5 imputed datasets using the default random forest method and 0 iterations (just for illustrative purposes), set a seed for reproducibility, and assign the output to `data` for use with *devMSMs*. This code takes some time to run. 

```{r}
s <- 1234

m <- 5

method <- "rf" 

maxit <- 0

imputed_data <- imputeData(data = data_wide, exposure = exposure, outcome = outcome, 
                           m = m, method = method, maxit = maxit, para_proc = FALSE, 
                           seed = s, read_imps_from_file = FALSE, 
                           home_dir = home_dir, save.out = save.out)

data <- imputed_data
```
  
We inspect the output to the console for any warnings from `mice()`.  
  
The mice object can now be assigned to `data` for use in the *deveMSMs* package (see *Workflows* vignettes).  
  
<br>

#### P2b. Read in as a list wide imputed data saved locally 
Alternatively, if a user has imputed datasets already created using a program other than *mice*, they can read in, as a list, files saved locally as .csv files (labeled “1”:m) in a single folder. This list can be assigned to `data` for use in the *deveMSMs* package (see *Workflows* vignettes).  
  
Below, we load in imputed data simulated from FLP, as an example.  
```{r}
# folder <- "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/testing data/continuous outcome/continuous exposure/imputations/"
# 
# files <- list.files(folder, full.names = TRUE, pattern = "\\.csv") #make sure pattern matches suffix of your data
# 
# data <- lapply(files, function(file) {
#   imp_data <- read.csv(file)
#   imp_data
# })

data("sim_data_imp_list", package = "devMSMs")

data <- sim_data_imp_list

```

Alternatively, in the case of no missing data, the user can read in a formatted, wide dataset assigned to `data` for use in the *deveMSMs* package (see *Workflows* vignettes).  
  
Below, we load in a single wide data frame simulated from FLP as an example.  

```{r}
data("sim_data_wide", package = "devMSMs")

data_wide <- sim_data_wide

data <- data_wide
```

<br>

### P3. Optional: Identify Exposure Epochs
Users have the option to specify exposure epochs, or meaningful periods of developmental time that many encompass the time points at which the exposure was measured. The user has the option to draw on theory and the structure of their data to specify developmental epochs of exposure that differ from the time points at which exposure was collected.  
  
To specify epochs, users utilize the optional `epochs` argument by providing a data frame that contains two variables: for epochs: provide, in quotations, a list of user-created names for each epoch; for values: as a list, for each named epoch, provide a single integer or a list of integers (from the exposure time points) that constitute each epoch. Each named epoch must have a corresponding value (but the values for each epoch can differ in their number of entries, as shown below). The user should ensure that all epoch values are included in the `exposure_time_pts` field.   
  
The exposure epochs will be arguments in the `fitModel()` and `compareHistories()` *devMSMs* functions (see *Workflows* vignettes) and their specification should be kept consistent throughout use of the package and its vignettes. THey will constitute the main effects variables when modeling the relation between exposure and outcome (*Workflows* vignettes Step 5a) and form the basis for estimating and comparing exposure histories (*Workflows* vignettes Step 5b). If no epochs are specified, the exposure time points will be used at the aforementioned steps.  
  
Below, we specify Infancy, Toddlerhood, and Childhood epochs that correspond to 6 and 15; 24 and 35; and 58 months, respectively.  
```{r}
epochs <- data.frame(epochs = c("Infancy", 
                                "Toddlerhood", 
                                "Childhood"), 
                     values = I(list(c(6, 15), 
                                     c(24, 35), 
                                     c(58)
                     ))) 
```

<br>

### P4. Recommended: Specify & Inspect Exposure Histories
Exposure histories are the units with which users will test their substantive hypotheses, and their construction should be determined by both theoretical and practical reasoning. We strongly recommend users verify and inspect exposure histories a priori in relation to their data and hypotheses.   
<br>

### P4a. Create high and low cutoff values for continuous exposures
First, for continuously distributed exposures (regardless of whether or not exposure epochs are specified), we recommend users indicate high and low cutoff values as an optional input to the `compareHistories()`) *devMSMs* function (see *Workflows* vignettes).  
  
To do so, they specify to `hi_lo_cut`, as a list, a quantile value (0-1) above which will be considered high levels exposure, followed by a quantile value (0-1) below which will be considered low levels of exposure (default is median split). These values may have to be revised following inspection of the sample distribution across the resulting exposure histories in the subsequent steps. These final values will be used in creating exposure histories in Step 5 of the *Workflows* vignettes.  
  
Below, we specify the 60th and 30th percentiles to demarcate high and low levels of economic strain exposure, respectively.  
```{r}
hi_lo_cut <- c(0.6, 0.3) 
```
<br>


### P4b. Specify hypotheses-relevant exposure histories 
We strongly recommend users be selective about which histories, or developmental sequences of high and low exposure (at exposure time points or epochs), are vital for testing their hypotheses. We recommend that the user estimates and compares only a subset of all possible exposure histories using the `reference` and `comparison` fields (rather than comparing all possible exposure histories). 

The user can specify a custom subset of exposure histories using the `reference` and `comparison` fields as optional inputs to the `compareHistories()` *devMSMs* function (see *Workflows* vignettes). To conduct these customized comparisons, users must provide at least one unique valid history (e.g., “l-l-l”) as a reference by, in quotations, provide a string (or a list of strings) of lowercase l’s and h’s (each separated by -), each corresponding to each exposure epoch (or time point), that signify the sequence of exposure levels (“low” or “high”, respectively).   

If you supply a reference history, in comparisons provide at least one unique and valid history for comparison by, in quotations, providing a string (or list of strings) of l’s and h’s (each separated by “-”), with each corresponding to each exposure epoch, that signify the sequence of exposure levels (“low” or “high”, respectively) that constitutes the comparison exposure history/histories to be compared to the reference in Step 5b of the *Workflows* vignettes. If you supply one or more comparisons, at least one reference must be specified. Each reference exposure history will be compared to each comparison history and all comparisons will be supplied for multiple comparison correction. If no reference or comparison is specified, all histories will be compared to each other in Step 5b of the *Workflows* vignettes.   

These final reference and comparison values established at this step should be used for estimating and comparing exposure histories in Step 5b of the *Workflows* vignettes. If there are  more than 4 exposure main effects (either as epochs or exposure time points), the user is required to select a subset of history comparisons (Step 5b of the *Workflows* vignettes), given that the base code (see the `hypotheses()` function from the *marginaleffects* package) cannot accommodate all pairwise history comparisons for more than 5 time points).  
  
Below, we specify low economic strain at all epochs ("l-l-l") as our reference event in comparison to high levels at all epochs ("h-h-h") as well as all histories that contain 1 dose of exposure to high economic strain at different epochs.  
```{r}
reference <- c("l-l-l")

comparison <- c("h-h-h", "l-l-h", "h-l-l", "l-h-l") 
```
  
<br>

### P4c. Inspect exposure histories and data
For all users, we highly recommend use of the helper `inspectData()` function (with the a complete dataset in long or wide format or imputed data in the case of missingness) to summarize exposure, outcome, and confounders and inspect the sample distribution among exposure histories. Based on any user-specified exposure epochs and high and low quantile values (for continuous exposures), this function outputs a table showing the sample distribution across all histories.  
  
We strongly suggest visually inspecting this table and revising the designation of epochs and/or high and low quantile values (for continuous exposures) until each history contains a reasonable number of participants. While there is no gold standard required number per history cell, users should guard against extrapolation beyond the scope of the data. For example, in our data, when using 75th and 25th percentile cutoffs, there were histories that represented less than two cases and thus we re-evaluated our cutoffs. Users may wish to revise any epoch designation and high and low cutoff values, where applicable. The function conducts summaries and history distribution inspection for each imputed dataset if imputed data are supplied. 
  
<br>

*insert Table 2*

<br>

The required inputs for `inspectData()` are: complete data (as a data frame in wide or long format, a list of imputed data frames in wide format, or a mids object), exposure (e.g., “variable”), and outcome (e.g., “variable.t”).   

Optional inputs are a home directory (if `save.out = TRUE`), epochs, high/low cutoff values for continuous exposures, and specification of reference and comparison histories. 
  
The helper `inspectData()` function outputs the following files into the home directory: a correlation plot of all variables in the dataset, tables of exposure and outcome descriptive statistics, and two summary tables of the confounders considered at each time point.

```{r}
inspectData(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, # required input
            ti_confounders = ti_confounders, tv_confounders = tv_confounders, # required input
            epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional input
            home_dir = home_dir, verbose = verbose, save.out = save.out) #optional input

```
Here, we see summaries of the data types as well as reasonable cell counts in each of our specified histories, for each imputed dataset. 
  
<br>


### References
Arel-Bundock, Vincent. 2023. marginaleffects: Predictions, Comparisons, Slopes, Marginal  Means, and Hypothesis Tests. 
https://CRAN.R-project.org/package=marginaleffects.

Burchinal, M., Howes, C., Pianta, R., Bryant, D., Early, D., Clifford, R., & Barbarin, O. (2008). Predicting Child Outcomes at the End of Kindergarten from the Quality of Pre-Kindergarten Teacher–Child Interactions and Instruction. Applied Developmental Science, 12(3), 140–153. https://doi.org/10.1080/10888690802199418  

Granger, E., Sergeant, J. C., & Lunt, M. (2019). Avoiding pitfalls when combining multiple imputation and propensity scores. Statistics in Medicine, 38(26), 5120–5132. https://doi.org/10.1002/sim.8355

Leyrat, C., Carpenter, J. R., Bailly, S., & Williamson, E. J. (2021). Common Methods for Handling Missing Data in Marginal Structural Models: What Works and Why. American Journal of Epidemiology, 190(4), 663–672. https://doi.org/10.1093/aje/kwaa225
  
Shah, A. D., Bartlett, J. W., Carpenter, J., Nicholas, O., & Hemingway, H. (2014). Comparison of Random Forest and Parametric Imputation Models for Imputing Missing Data Using MICE: A CALIBER Study. American Journal of Epidemiology, 179(6), 764–774. https://doi.org/10.1093/aje/kwt312
  
Vernon-Feagans, L., Cox, M., Willoughby, M., Burchinal, M., Garrett-Peters, P., Mills-Koonce, R., Garrett-Peiers, P., Conger, R. D., & Bauer, P. J. (2013). The Family Life Project: An Epidemiological and Developmental Study of Young Children Living in Poor Rural Communities. Monographs of the Society for Research in Child Development, 78(5), i–150.  
  
van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011. “mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software 45 (3): 1–67. https://doi.org/10.18637/jss.v045.i03.



