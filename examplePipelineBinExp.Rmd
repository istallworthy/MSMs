---
title: "examplePipeline for the devMSMs package"
author: "Isa Stallworthy", "Meriah DeJoseph", "Emily Padrutt", "Daniel Berry"
date: 10/27/2022
output: html_document
---

This code provides a framework for testing the putative causal effects of exposure histories on outcome(s) that measured at a single final time point. 

The most updated version of this package can be found at: https://github.com/istallworthy/MSMs 

Example pipeline running the MSM functions in sequence based on causal questions centered on the Family Life Project (FLP). 


### Load package
```{r}
# install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
library(devtools)
library(roxygen2)
load_all()

devtools::document()

# traceback()
```

### 1. Provide user inputs to create the msm object and format data
Please provide user input for the following fields to create an msm object that will be used in all future functions, format data, and create the required directories. 

```{r}
# source("") #string

# data=read.csv("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/MSM_ALL_LONG_bin_8.25.20.csv")
# data[data==-9999]=NA
# data$INR_bin=ifelse(data$INR>median(data$INR, na.rm=TRUE), 1, 0)
# write.csv(data, "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/MSM_ALL_LONG_bin_8.25.20.csv")

object <- msmObject(
  
  ##### Directories Information (required) ####
  # directories to long dataset 
  # data_path = "~/Library/CloudStorage/Box-Box/BSL General/Meriah/TSST MSM PAPER/MSMs/!MSM package test_10.17.22/MSMs-main/MSM_ALL_LONG_8.25.20.csv", #string
  data_path ="/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/MSM_ALL_LONG_bin_8.25.20.csv",
  
  # home_dir = "~/Library/CloudStorage/Box-Box/BSL General/Meriah/TSST MSM PAPER/MSMs/!MSM package test_10.17.22/MSMs-main", #string
  home_dir = "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/", #string
  
  
  #### Dataset Information (required) ####
  #individual identifier 
  ID = "s_id", #string
  
  #time points of interest and name of time variable in long dataset
  time_pts=c(6, 15, 24, 35, 58, 90, 154), #list of integers
  time_var="WAVE", #string
  
  #marker for missing data
  missing=-9999, #string or integer(s)
  
  #list out all variables that are time-varying; default is none
  time_varying_variables= c("AGE", "WIND", "PCX_POS", "PCX_NEG", "BSI", "INR_bin", "INR", "TIMEofDAY", "CORTBASE", "sAABASE", "ESETA1", "HOMEETA1", "CTSETA1"),
  
  #list all continuous variables here; all others assumed ordinal for imputation; default is none 
  continuous_variables=c("GrosPay1", "RMomAgeU", "RMAge1st", "SwghtLB","RWghtLb", "IBQDnovm", "MDI", "AGE", "WIND", "PCX_POS", "PCX_NEG", "BSI", "TIMEofDAY", "CORTBASE", "sAABASE", "ESETA1", "HOMEETA1", "CTSETA1", "INR"),
  
  #list any covariates that are factors here; default is none, or that all variables are continuous 
  factor_covariates=c("NC","TcSex2", "TcBlac2", "INR_bin"), #list of characters
  
  
  #### Imputation Specification (optional) ####
  #number of imputations for step 4; default is 5 --add any other imputation parameters here?
  m=2, #integer
  
  
  #### Exposure Information (required) ####
  #exposures of interest that potentially have causal effects on outcomes and exposure time point(s) (corresponding to values in time_var)
  exposures=c("INR", "HOMEETA1"), #list of strings
  # exposure_time_pts=c(6, 15, 24, 35, 58, 90, 154), #list of integers
  exposure_time_pts=c(6, 15), #list of integers

  
  #### Balancing Information (optional) ###
  balance_thresh=0.12, #correlation value above which covariates are not considered balanced with respect to exposure/outcome;used for finding potential confounds and asessing balance; default set to 0.12 from Stuart et al.
  
  weights_percentile_cutoff=0.95, #percentile cutoff value for truncating weights to avoid heavy tails; default is 0.95; 
  
  
  #### History Comparison Information (required & optional) ####
   #developmental time periods (and their corresponding time points) that constitute regime/history units of interest --these should be meaningful periods of time that may collapse over exposure time points to result in more manageable combinations for exploring effects of exposure histories (e.g, chronically high exposure vs. exposure only in infancy) on outcomes
  exposure_epochs=data.frame(epochs=c("Infancy", "Toddlerhood", "Childhood"), #user-created names of epochs as a list of strings
                             values=I(list(c(6,15), c(24,35), c(58, 90, 154)))), #time point(s) encompassed in each epoch that correspond to data as a list of numeric lists
  
    #inspect the following exposure levels denoting all possible histories of high ("h") and low ("l") exposure over the epochs
  # apply(gtools::permutations(2, nrow(object$exposure_epochs), c("l", "h"), repeats.allowed=TRUE), 1, paste, sep="", collapse="-")
  #select one of the above histories, or permutations of high ("h") and low ("l") levels of exposure (one leve for each of the exposure epochs) as your reference event, or the history/sequence to which you would like to compare the other histories  
  reference="l-l-l", #optional: set a reference history of high ("h") and low ("l") levels of exposure separate by a "-"; the default is set to the history denoting low ("l") exposure at all time points; character string of "h" and "l" separated by a "-" 
  comparisons="", #optional: set a comparison history from list above; the default is to include all non-reference histories as comparisons; character string of "h" and "l" separated by a "-" 
  
  hi_cutoff=.75, #optional integer value for quantile value constituting the threshold for "high" levels of an exposure; default set to 0.75
  lo_cutoff=.25, #optional integer value for quantile value constituting the threshold for "low" levels of an exposure; default set to 0.25

  mc_method="BH", #optional method for multiple comparison correction for linear hypothesis tests; default is Benjamini-Hochburg, options are "holm", "hochberg","hommel", "bonferroni", "BH", "BY", "fdr", "none" (see stats::p.adjust() documentation)
  
  
  #### Outcome Information (required) ####
  #outcomes of interest and outcome time point
  # outcomes=c("CORTBASE", "sAABASE"), #list of strings 
  outcomes=c("CORTBASE"), #list of strings 
  outcome_time_pt=154, #integer value
  
  
 #### Inclusion Information (optional) ####
  #list any variables for mandatory inclusion in balancing for theoretical or other reasons (regardless of their correlations with exposure/outcome); if it is a time-varying covariate the code will include it in the forms for all time points;  can specify time after "."; default set to none
  mandatory_keep_covariates=c("HOMEETA1"),
  
  
  #### Exclusion Information (optional) ####
  #list out any variables that should be excluded from consideration as confounds via balancing based on practical or theoretical reasons; if it is a time-varying covariate the code will include it in the forms for all time points with the exception of in its own forms if it is an exposure; can specify time after "."; default set to none; will over ride mandatory_keep_covariates
  exclude_covariates=c("Index1", "WAVE2", "WAVE3", "LIVEVID", "SUBSTRESS", "TANNERALL", "AGE.6", "INR_bin"), #list of characters
  
  #list any time-varying variables that may get imputed but should NOT be present in the dataset because of planned missingness design with ".time_pt" format
  time_var_exclude=c("PCX_POS.58", "PCX_NEG.58", "PCX_POS.90", "PCX_NEG.90", "PCX_POS.154", "PCX_NEG.154", "CORTBASE.58", "CORTBASE.90", "sAABASE.58","sAABASE.90", "TIMEofDAY.6", "TIMEofDAY.15", "TIMEofDAY.24", "TIMEofDAY.35", "TIMEofDAY.58", "TIMEofDAY.90","TIMEofDAY.15", "WIND.15", "WIND.24", "WIND.154", "BSI.35","BSI.90", "BSI.154"), #default set to none
  
  #when creating a balancing formula at each time point, we need to remove potential colliders at that time point. list potential colliders (i.e., variables that could cause both the treatment and the outcome) at each time point below. these will be EXCLUDED from balancing only at the time point of the exposure and outcome, and at lagged values if specified by the user, as balancing on colliders can lead to problematic outcomes; default is none; will override mandatory_keep_covariates
  potential_colliders=data.frame(exp_out_pair=c("HOMEETA1-CORTBASE", "ESETA1-CORTBASE"), #names of exposure-outcome pair separated by "-", no spaces; list of strings
                                 colliders=I(list(c("AGE"), c("HOMEETA1.154", "AGE"))), #list of colliders for each exposure-outcome pair; list of lists of strings
                                 exclude_lags=c("T", "F")), #whether or not lagged values of listed colliders, if they are time-varying, should also be excluded ("Y"=exclude lagged values); one entry per exp-out pair; list of strings
                                 
 
 ### Plotting Information (optional) ###
 #optional list of alternative labels for exposures that will be used only for plotting
   # exposure_labels=c("Home Resources", "Conflict in the Home", "Threat"), 
    exposure_labels=c("Income-to-needs-ratio"), 

   
  #optional list of alternative labels for outcomes that will be used only for plotting
   # outcome_labels=c("Cortisol","Salivary Alpha Amalayse"),
    outcome_labels=c("Cortisol"),

 #optional string ("h" or "l") denoting which level of exposure at each exposure epoch to use when calculating dose (default is "h")
   dose_level="l",
 
 #the plots color data by dose (i.e., number of epochs) of high exposure; optional list of colors (equal to number of epochs +1) or brewer palette (see RColorBrewer::display.brewer.all() or https://r-graph-gallery.com/38-rcolorbrewers-palettes.html) 
   colors=(c("Dark2")) #list of string(s); default is 'Dark2'); c("blue4", "darkgreen", "darkgoldenrod", "red2")
)



#Creates the necessary directories in your home directory and formats your dataset
data <- formatDataStruct(object)
```




### 2. Identify covariate confounders of the relationship(s) between exposure(s) and outcome(s) to be used to create balancing weights
MSMs focus on the covariates that are associated with both exposure(s) and outcome(s) and thus could act as a confounding variable. More specifically, potential confounds for a given exposure-outcome pair at a given time point include any covariate: correlated with the exposure at that time point, lagged values of the exposure, or correlated with the outcome. The method assumes that you have included all possible confounding variables as columns in your dataset.

When creating the msm object, the user must specify which covariates are time-varying ('time_varying covariates'). This code identifies all possible confounding variables and creates a simplified dataset for imputation. This is primarily a data-driven approach to identifying potential confounds. However, the user has the option to specify any covariates to be sure to exclude ('exclude_covariates') as well as those to include ('keep_covariates'), for theoretical or other reasons. The user can include time-invariant or time-varying covariates in these fields. Append ".x" (e.g., "INR.6) to the covariate to indicate a time-varying variable at a specific time point only, otherwise the condition will be assumed to all time points of any time-varying variables. 

*note from IS: not sure how to suppress the html code from console (i.e., to only save it out and not print when running)
```{r}

#identifies the total number covariates available in the dataset and prints them for user inspection
potential_covariates <- identifyCovariates(object, data)

#of those covariates, variables at the exposure time point or prior that correlate with either the exposure at that time point or the outcome at the outcome time point  and thus could be potential confounders
covariates_to_include <- identifyPotentialConfounds(object)

```


### 3. Impute data to account of missingness
ONlY run this code when needed! Creates 'm' imputed datasets using the Amelia package (Honaker, King, & Blackwell, 2011) and functions to eventually generate weights for each imputed dataset (that will be combined at the end). This code takes a few minutes to run per imputation. 

The parameter 'read_imps_from_file' will allow you to read imputed data in from local storage (="yes") so as not to have to re-run this imputation code or impute data (="no"; default). The number of imputed datasets is specified as 'm' in the msm object. 
```{r}
#create final dataset for imputations with only the necessary variables
data_to_impute <- dataToImpute(object, covariates_to_include)

#creates imputed datasets and saves them out
imputed_datasets <- imputeData(object, data_to_impute, read_imps_from_file="yes")

```


### 4. Create balancing formulas and balancing weights for each exposure-outcome pair at each exposure time point 
Making sure imputed datasets have the appropriate variables and are formatted in the "wide/long" format needed to create the balancing weights. This code also saves out a dataset structured for future modeling. 

Balancing weights focus on the relationship between potential confounding variables and each exposure. The following code creates formulas relating all appropriate concurrent and lagged covariates to each exposure at each time point. These will be used to create balancing weights in subsequent steps. This step is crucial --the user is advised to thoroughly inspect each formula and consider excluding potential collider variables at the msm object stage. 

This step creates balancing weights using the Covariate Balancing Propensity Score (CBPS) package and approach (Imai & Ratkovic, 2014; Fong, 2022). Please see the CBPS package documentation (https://cran.r-project.org/web/packages/CBPS/CBPS.pdf) for more information about the other optional user parameters. The weights creation code takes a little while to run. The parameter, 'read_in_from_file', allows you to instead read in weights that have already been saved locally (="yes") or to create weights (="no"; default).
```{r}

#creates wide/long datasets for each imputation
wide_long_datasets <- formatForWeights(object, data, imputed_datasets)

#creates formulas for each exposure-outcome pairing at each time point and saves and prints them for user inspection
forms <- createForms(object, wide_long_datasets, covariates_to_include)

#creates balancing weights for each exposure-outcome pair at each exposure time point 
weights_models <- createWeights(object, wide_long_datasets, forms, read_in_from_file="no") 

```



### 5. Assess balance and determine covariates for which balance could not be achieved 
This code evaluates which covariates are still correlated with exposures (i.e., for which balancing was not successful) above the designated threshold which the user specified in the msm object, after averaging across imputed datasets, and saves out a list of those covariates to include in your final model. 
IS not sure how to suppress console output....
```{r}

#assesses covariate balance and produces figures for user inspection
unbalanced_covariates_for_models <- assessBalance(object, data, weights_models)

```


### 6. Condense and truncate weights to create one per person per exposure-outcome pair
This code multiplies weights across time points and averages across imputed datasets to condense down to final individual weights per exposure-outcome pair for modeling.

Given that weights with heavy tails can be problematic, this code caps weight values at a designated percentile (that the user has the option to specify with 'percentile_cutoff' when creating the msm object), populating all weights larger than that value with that percentile value. These weights can now be used in the formal marginal structural model, or weighted model. This code cycles through 2 additional cutoff values adjacent to the one specified by the user in order to conduct recommended sensitivity analyses with subsequent models. 

You can stop here if you have specific MSM models you want to fit or you can continue on to fit a standard single-outcome model. We also suggest conducting sensitivity/robustness tests by creating weights using several percentile cutoff values and ensuring your subsequent models produce similar results regardless of which percentile cutoff you used. 
```{r}
#condenses weights across imputed dataset and time point 
data_for_model_with_weights <- condenseWeights(object, weights_models)

#truncate weights 
data_for_model_with_weights_cutoff <- truncateWeights(object, data_for_model_with_weights)

```





Please choose either steps 7-8 if you have a single value outcome at a single time point or step 9 if your outcome exhibits a growth process at at the outcome time point. 


### 7. For a single outcome value: Determine best-fitting marginal outcome model
This code fits a series of weighted generalized linear models using the 'survey' package (Lumley, 2021) using the final truncated weights to separately examine the putative causal effects of each exposure on each (single time point) outcome, testing for the need for any covariates that were not successfully balanced. Fit is assessed using AIC. 

The current approach retains main effects of each exposure epoch for the purpose of examining different histories in the next step, and tests for all 2-way interactions between exposure main effects. 
Model dictionary:
*m0 baseline model with only main effects for each exposure epoch
*m1 full covariate model adding in all unbalanced covariates (if any)
*m2 covariate model including only significant covariates (retaining all epoch main effects)
*m3 full interaction model adding in all possible epoch main effect interactions
*m4 final model including all epoch main effects and only the significant covariates and interactions 

```{r}

#code to fit a series of baseline, covariates, and interaction weighted models to determine which is best fitting; prints only models with user-specified weight cutoff values (sensitivity checks output into folder)
all_models <- fitModel(object, data_for_model_with_weights_cutoff, unbalanced_covariates_for_models)

#determine best-fitting model  --ignore orange warnings here, could suppress them?
best_models <- assessModel(object, all_models)

```


### 8. For a single outcome value: Compare effects of exposure histories on outcomes, correct for multiple comparisons, and plot results 
This code fits a series of linear hypothesis tests using the 'car' package (Fox & Weisberg, 2019) to examine the effects of different combinations of exposures, or histories, on each outcome using the best-fitting model from the previous step. 

Because we use a continuous approach to modeling exposures, the user has the option to specify cutoffs for values of the exposures that will be considered high ("hi_cutoff") and low ("lo_cutoff") in the msm object. The user also has the option to specify a reference history ("reference") and an optional comparison history ("comparisons") on which to base comparisons, and which multiple comparison correction method ('method') to implement. 

This code uses predicted values to plot the effects of the different exposure histories and resulting comparisons for each exposure-outcome pairing. Use the results from the above step to determine which exposure histories are significantly different from one another. Plots display outcome on the x-axis and exposure history on the y-axis, colored by dose (0-n(length of epochs)).The user has the option to provide more formal labels for the exposure(s) ('exposure_labels') and outcome ('outcome_labels') in the msm object to be used for plotting.  
```{r}

#conducts linear hypothesis testing using the best-fitting model for each exposure-outcome pairing
history_comparisons <- compareHistories(object, best_models)

#adjusts p-values for multiple comparisons with an optional user-specified method with Benjamini-Hochburg as the default
signifcant_comparisons <- mcCorrection(object, history_comparisons) #maybe try to suppress output?

#saves out plots of exposure histories for each out exposure-outcome pairing, colored by dosage of exposure 
plotResults(object, best_models)
```


Implement this step if you have a growth process for your outcome at the outcome time point 

### 9. For an outcome growth process: Create an MPlus template for fitting weighted, more complex weighted models
This code creates a template for fitting weighted more complex growth models in MPlus in the form of a .inp file. The code defaults to estimating slope (s) and quadratic (qd) growth and estimates contrasts examining the differences between different exposure histories. 

The user is advised to independently apply the appropriate transformations to their outcome measure and establish its appropriate functional form. They can then provide a .csv file to this function to create a template of an MPlus .inp file for fitting a growth model. The user can then modify the template .inp file as needed prior to running. 

[Meriah to add more info about what these models do]

assumes same missing indicator as original data
abridged dataset --only include necessary variables?
does up to 3-way interactions only
data must have outcomes in long form but exposures in wide form --check to see if there's a wide/long version of data saved out somewhere to guide the user?

Note for Meriah: any other variables/input fields that the user should specify? anything else to save out along the way?

Data should be modeled after the csv: TSST_allTxseq_LONG3.csv (which i manually put in the 'for Mplus' folder) which the user should create independently

The user can also view unbalanced covariates by time and averaged across time in the 'balance/post-balance correlation values/' folder. 
```{r}
#inspect the following exposure levels denoting all possible histories of high ("h") and low ("l") exposure over the epochs
# apply(gtools::permutations(2, nrow(object$exposure_epochs), c("L", "H"), repeats.allowed=TRUE), 1, paste, sep="", collapse="")


mplusObject <- mplusObject(
  #location of your data file in long format including final truncated weights (labeed exposure-outcome)
  data_file=paste0(object$home_dir, "for Mplus/TSST_allTxseq_LONG3.csv"),
  
  #list your outcomes and (optionally) any corresponding covariates you want included in each respective model
  additional_covariates=data.frame(outcomes=c("lnCORTR", "sqrtSAAR"), #names of outcomes that are in long form (MLD)
                        additional_covariates=I(list(c("Timeofday_c"), c("SubStressR_mn","Timeofday_c")))) #like methods covariates that don't need balancing
)


#create .imp files and header-less .csv files for each exposure-outcome pairing. This code adapts content from Gottfredson et al., 2019 via the aMNLFA package. 
makeMplusInputs(object, mplusObject)


#automatically run all inputs in the Mplus folder after you have manually modified/checked them
MplusAutomation::runModels(replaceOutfile = "never",  target = paste0(object$home_dir, 'for MPlus/original/'))

```


References --add Gottfredson, Stuart, Imai, etc. 
```{r}
print(citation("survey"))
print(citation("CBPS"))
print(citation("Amelia"))
print(citation("car"))

```

