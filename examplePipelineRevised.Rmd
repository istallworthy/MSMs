---
title: "Recommended Workflow for using devMSMs with Longitudinal Data"
author: "Isabella C. Stallworthy", "Meriah L. DeJoseph", "Emily R. Padrutt", "Noah Greifer", "Daniel Berry"
date: "`r Sys.Date()`"
output: html_document
---

Please see this manuscript for a full conceptual and practical introduction to MSMs in the context of developmental data. Please see the vignettes on the *devMSMs* website for step-by-step guidance on the use of this code: https://istallworthy.github.io/devMSMs/index.html. 

The code in each code chunk is set up identifying all possible inputs to each function (required and optional) to aid the user's use of the full range of package functionality. Example possible values for the optional input are shown for each function, including a NULL/NA option if the user does not wish to specify an optional input. The user should select one of each optional input values. Alternatively, the user could modify the call to the function and remove the optional input argument(s) entirely. 

Please see the website vignettes and/or type `?functionName` into the console for more guidance on the arguments for each function. These two sources should match but let me know if you see discrepancies. 


## Getting started
Until *devMSMs* is available on CRAN, you will need to install it directly from Github (https://github.com/istallworthy/devMSMs), as shown below.  

You will always need to install the devMSMs helper functions from Github (*devMSMsHelpers*) if you wish to conduct the preliminary steps below. 

```{r}
install.packages("devtools", quiet = TRUE)
library(devtools)

install_github("istallworthy/devMSMs", quiet = TRUE)
library(devMSMs)

install_github("istallworthy/devMSMsHelpers", quiet = TRUE)
library(devMSMsHelpers)

#note: if I update Github to fix something, you may need to first uninstall the package(s) by running the following code:
# remove.packages("devMSMs") or 
# remove.packages("devMSMsHelpers")
#prior to re-installing using the code above. Sorry, this is annoying! There may also be a short lag between when I update something on Github and when it becomes available for install. 

```


## Specifying Required Package Core Inputs
The user should change all fields in this code chunk to match their home directory and wide data.

```{r}

#set seed for reproducibility 
set.seed(1234)

#required if you wish to use save.out = TRUE in the functions
home_dir <- NA
home_dir <- '/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/isa' #note: no / after

#required
exposure <- "ESETA1"

#required
exposure_time_pts <- c(6, 15, 24, 35, 58)

#required
outcome <- "StrDif_Tot.58"

#required; list in wide format
tv_confounders <- c("SAAmylase.6","SAAmylase.15", "SAAmylase.24",
                    "MDI.6", "MDI.15",                                            
                    "RHasSO.6", "RHasSO.15", "RHasSO.24","RHasSO.35", "RHasSO.58",                                         
                    "WndNbrhood.6","WndNbrhood.24", "WndNbrhood.35", "WndNbrhood.58",                                       
                    "IBRAttn.6", "IBRAttn.15", "IBRAttn.24",                                   
                    "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58",                                           
                    "HOMEETA1.6", "HOMEETA1.15", "HOMEETA1.24", "HOMEETA1.35", "HOMEETA1.58",                               
                    "InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58",                         
                    "ESETA1.6", "ESETA1.15", "ESETA1.24", "ESETA1.35", "ESETA1.58",  #exposure variables required               
                    "CORTB.6", "CORTB.15", "CORTB.24",                                                                  
                    "EARS_TJo.24", "EARS_TJo.35",                                        
                    "LESMnPos.24", "LESMnPos.35",                                  
                    "LESMnNeg.24", "LESMnNeg.35",       
                    "StrDif_Tot.35", "StrDif_Tot.58",    
                    "fscore.35", "fscore.58"
                    # , "ESETA1.6:B18Raw.6", "ESETA1.6:B18Raw.15:RHasSO.6", "state:EARS_TJo.35" #testing interactions
) 

#required
ti_confounders <- c("state", "BioDadInHH2", "PmAge2", "PmBlac2", "TcBlac2", "PmMrSt2", "PmEd2", "KFASTScr",
                    "RMomAgeU", "RHealth", "HomeOwnd", "SWghtLB", "SurpPreg", "SmokTotl", "DrnkFreq",
                    "peri_health", "caregiv_health", "gov_assist"
                    #, "state:SmokTotl", "PmAge2:PmBlac2", "PmAge2:PmEd2" #testing interaction terms
)
```


## STEP P: Preliminary Steps for Reading in, Formatting, & Inspecting Data
We advise users implement the appropriate preliminary steps, with the goal of assigning to 'data' one of the following wide data formats (see Figure 1) for use in the package:  
  
* a single data frame of data in wide format with no missing data  
  
* a mids object (output from mice::mice()) of data imputed in wide format  
  
* a list of data imputed in wide format as data frames.  

<br>
<br>

Users have several options for reading in data. They can begin this workflow with the following options:  
<br>
* long data with missingness can can be formatted and converted to wide data (P1a) for imputation (P2)
  
* wide with missingness can be formatted (P1b) before imputing (P2)
  
* data already imputed in wide format can be read in as a list (P2). 

* data with no missingness (P3)
  
<br>


### P1. Data with Missingness
Most data from humans will have some degree of missing data. 

#### P1a. A single data frame of long data with missingness
Users can begin with a single data frame in long format (with missingness). Users have the option of formatting their long data if their data columns are not yet labeled correctly using the `formatLongData()` helper function.    


```{r}
data_long <- read.csv("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/testing data/continuous outcome/continuous exposure/FLP_long_missing_unformatted.csv", #add path to your long data file here if you want to begin with long data
                      header = TRUE)

data_long_f <- formatLongData(data = data_long, exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, 
                              time_var = "Tage", #list original time variable here if it's not "WAVE"
                              id_var = "S_ID", #list original id variable here if it's not "ID"
                              missing = -9999, #list missing value here
                              factor_confounders = c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                                                     "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                                                     "RHasSO"), #list factor variables in long format here
                              integer_confounders = c("KFASTScr", "PmEd2", "RMomAgeU", "SWghtLB", "peri_health", "caregiv_health" , 
                                                      "gov_assist", "B18Raw", "EARS_TJo", "MDI"), #list any integer variables in long format here
                              home_dir = home_dir, save.out = TRUE) 
```
  
  
Users with correctly formatted data in long format have the option of using the following code to transform their data into wide format, to proceed to using the package (if there is no missing data) or imputing (with < 20% missing data MAR).They may see some warnings about *NA* values.    
```{r}
v <- sapply(strsplit(tv_confounders[!grepl("\\:", tv_confounders)], "\\."), "[", 1)
v <- v[!duplicated(v)]

library(stats)
data_wide <- stats::reshape(data = data_long_f, 
                            idvar = "ID", #list ID variable in your dataset
                            v.names = v, 
                            timevar = "WAVE", # list time variable in your long dataset
                            times = c(6, 15, 24, 35, 58), # list all time points in your dataset
                            direction = "wide")
data_wide <- data_wide[, colSums(is.na(data_wide)) < nrow(data_wide)]

# data_wide2=data_wide
# data_wide2=data_wide2[order(names(data_wide2))]
```

 
#### P1b. A single data frame of wide data with missingness

Alternatively, we could start with a single data frame of wide data with missingness that is already formatted, and below we read in simulated wide FLP data with missingness.   
```{r}
data_wide <- read.csv("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/Tutorial paper/merged_tutorial_filtered.csv", #add path to your wide, formatted data file here 
                      header = TRUE)

factor_confounders <- c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                        "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                        "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58") #list factor variables in wide format here

integer_confounders = c("KFASTScr", "PmEd2", "RMomAgeU", "SWghtLB", "peri_health", "caregiv_health" , 
                        "gov_assist", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58", 
                        "EARS_TJo.24", "EARS_TJo.35", "MDI.6", "MDI.15") #list any integer variables in wide format here

data_wide_f <- formatWideData(data = data_wide, exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, 
                              id_var = "ID", #list original id variable here if it's not "ID"
                              missing = NA, #list missing value here
                              factor_confounders = factor_confounders,
                              integer_confounders = integer_confounders,
                              home_dir = home_dir, save.out = TRUE) 

data <- data_wide_f
```


### P2. Impute Missing Data in Wide Format
Most data collected from humans will have some degree of missing data.  

#### P2a. Multiply impute single wide, formatted data frame using mice
Users have the option of imputing their wide, formatted data using the *mice* package, for use with *devMSMs*.     
```{r}
set.seed(1234)

#optional seed for reproducibility 
s <- NA 
s <- 1234 #empirical example

#optional; number of imputations (default is 5)
m <- NA
m <- 5 #empirical example

#optional; provide an imputation method pmm, midastouch, sample, cart , rf (default)
method <- NA
method <- "rf" #empirical example

#optional maximum iterations for imputation (default is 5)
maxit <- NA
maxit <- 5 #empirical example

imputed_data <- imputeData(data = data_wide, exposure = exposure, outcome = outcome, 
                           m = m, method = method, maxit = maxit, para_proc = TRUE, 
                           seed = s, read_imps_from_file = FALSE, 
                           home_dir = home_dir, save.out = TRUE)


#if you want to use the package with imputed data
data <- imputed_data

summary(mice::complete(data, 1))
```

```{r}
# row.names(data_wide) <-NULL
# row.names(data_wide2) <-NULL
# 
# data_wide <- data_wide[order(names(data_wide))]
# data_wide2 <- data_wide2[order(names(data_wide2))]
# 
# identical(data_wide, data_wide2)
# 
# sapply(data_wide2, class) 
# 
# integer_covars <- c("ID", "KFASTScr", "PmEd2", "RMomAgeU", "SWghtLB", "peri_health", "caregiv_health" , "gov_assist", "B18Raw.15", "B18Raw.24", "B18Raw.58",      "B18Raw.6", "EARS_TJo.24", "EARS_TJo.35", "MDI.15", "MDI.6")
# data_wide2[, integer_covars] <- as.data.frame(lapply(data_wide2[, integer_covars], as.integer))
# 
# library(dplyr)
# all_equal(data_wide, data_wide2)
# 
# all.equal(data_wide, data_wide2)
# 
# #testing
# seed = 1234
# data_to_impute <- tibble::tibble(data_wide)
# # data_to_impute <- tibble::tibble(data_wide2) #from long
# imputed_datasets <- mice::mice(data_to_impute, 
#                                m = m, 
#                                method = method, 
#                                maxit = maxit,
#                                print = F,
#                                seed = seed)
# summary(mice::complete(imputed_datasets, 1))

```

Alternatively, users could read in a saved mids object for use with *devMSMS*.    
```{r}
imputed_data <- readRDS("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/testing data/continuous outcome/continuous exposure/FLP_wide_imputed_mids.rds") # final imputations for empirical example; place your .rds file in your home directory and change the name of file here

#if you want to use the package with imputed data
data <- imputed_data

#optional: extract first imputed dataset for inspection
library(mice)
data <- mice::complete(imputed_data, 1) #just for testing purposes
anyNA(data) #make sure this is false, meaning no NAs
```




#### P2b. Read in as a list of wide imputed data saved locally 
Alternatively, if a user has imputed datasets already created using a program other than *mice*, they can read in, as a list, files saved locally as .csv files (labeled “1”:m) in a single folder. 
  
```{r}
folder <- "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/testing data/continuous outcome/continuous exposure/imputations/" # these are final imputations for empirical example; change this to match your local folder

files <- list.files(folder, full.names = TRUE, pattern = "\\.csv") #make sure pattern matches suffix of your data

#if you want to use the package with a list of imputed data from above
data <- lapply(files, function(file) {
  imp_data <- read.csv(file)
  imp_data
})

#check for character variables
any(as.logical(unlist(lapply(data, function(x) { #if this is TRUE, run next lines
  any(sapply(x, class) == "character") }))))
names(data[[1]])[sapply(data[[1]], class) == "character"] #find names of any character variables
data <- lapply(data, function(x){
  x[, "state"] <- factor(x[, "state"], labels = c(1, 0)) #run this for each variable that needs char -> factor
})

#make factors; list factor variables here
factor_covars <- c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                   "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                   "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58")

data <- lapply(data, function(x) {
  x[, factor_covars] <- as.data.frame(lapply(x[, factor_covars], as.factor))
  x })

```

### P3. Data with no Missingness
Users with no missing data can refer to Step P1a to format their long data and transform it to wide or Step P1b to format their wide data for use with *devMSMs*.  
 
```{r}
data <- read.csv('/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/testing data/continuous outcome/continuous exposure/FLP_wide_imputed.csv')

data_wide_f <- formatWideData(data = data_wide, exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, 
                              id_var = "ID", #list original id variable here if it's not "ID"
                              missing = NA, #list missing value here
                              factor_confounders = c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                                                     "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                                                     "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58"), #list factor variables in wide format here
                              integer_confounders = c("KFASTScr", "PmEd2", "RMomAgeU", "SWghtLB", "peri_health", "caregiv_health" , 
                                                      "gov_assist", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58", 
                                                      "EARS_TJo.24", "EARS_TJo.35", "MDI.6", "MDI.15"), #list any integer variables in wide format here
                              home_dir = home_dir, save.out = TRUE) 

data <- data_wide_f
```



Having read in their data, users should now have have assigned to `data` wide, complete data as: a data frame, mice object, or list of imputed data frames for use with the *devMSMs* functions.  

### P4. Optional: Identify Exposure Epochs
Users have the option of specifying exposure epochs, or menaingful units of developmental time that encompass the time points at which exposure was measured. We recommend users specify exposure epochs consistently throughout this workflow.  

```{r}
#change this to match your data/theory 
epochs <- data.frame(epochs = c("Infancy", #list user-specified names
                                "Toddlerhood", 
                                "Childhood"), 
                     values = I(list(c(6, 15), #list corresponding time points from data
                                     c(24, 35), 
                                     c(58)
                     ))) 
```


### P5. Recommended: Specify and Inspect Exposure Histories
Regardless of data formatting and exposure epochs, we highly recommend users inspect their data and the distribution of their sample throughout the exposure histories that will be used in subsequent steps.  

#### P5a. Create high and low cutoff values for continuous exposures
For continuously distributed exposures, users have the option to specify cutoffs for demarcating high and low levels of exposure, respectively. We recommend users specify high and low cutoffs consistently throughout this workflow.  

```{r}
#optional list of quantiles specifying high and low cutoff values, respectively, for continuous exposures; default is median
hi_lo_cut <- c(0.6, 0.3) #empirical example 
```


#### P5b. Specify hypotheses-relevant exposure histories
We recommend users specify the exposure histories that are relevant for testing their hypotheses. We suggest users specify reference and comparison histories consistently throughout this workflow.  

```{r}
#optional reference history (required if comparisons are specified)
reference <- NULL
reference <- "l-l-l" #empirical example final choice
reference <- c("l-l-l", "l-l-h")

#optional comparison history/histories
comparison <- NULL 
comparison <- "h-h-h" #single
comparison <- c("h-h-h", "h-l-l", "l-l-h", "h-h-l", "l-h-h") #empirical example final choice
```


#### P5c. Inspect exposure histories and data
We strongly recommend all users inspect their data and history distribution using the `inspectData()` helper function.   
```{r}
inspectData(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, # required input
            ti_confounders = ti_confounders, tv_confounders = tv_confounders, # required input
            epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional input
            home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional input

```


## PHASE 1: Confounder Adjustment
The first phase of the MSM process is focused on eliminating confounding of the relation between exposure and outcome.  

### STEP 1: Create Full Balancing Formulas & Conduct Pre-Balance Checking
The first step is to create full balancing formulas that reflect all measured confounders at each exposure time point.  

#### 1a. Create full balancing formulas at each exposure time point
Users have the option to specify concurrent confounders to retain and we recommend doing so consistently throughout this workflow. 

```{r}
#optional concurrent confounders
concur_conf <- NULL #empirical example 
concur_conf <- "B18Raw.15"

#optional custom formulas
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("full_form-6" = as.formula("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_assist"),
               "full_form-15" = as.formula("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_assist")
) 
custom <- NULL #empirical example 


#required
type <- "full"

#all inputs
full_formulas <- createFormulas(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, #required
                                type = type, ti_confounders = ti_confounders, tv_confounders = tv_confounders, #required
                                concur_conf = concur_conf, custom = custom, #optional
                                home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


#### 1b. Conduct exploratory pre-balance assessment
The next step is to examine initial imbalance between confounders and exposure prior to IPTW weighting. Users have the option to specify balance threshold(s), which we recommend doing consistently throughout this workflow.  

```{r}
#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) #empirical example 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58", "PmEd2") #empirical example 

#required
type <- "prebalance"
formulas <- full_formulas

#all inputs
prebalance_stats <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, 
                                  outcome = outcome, type = type, formulas = formulas, #required
                                  balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                  home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


### STEP 2: Create Simplified Balancing Formulas & Determine Optimal Weighting Method
The next step is to specify shorter, simplified balancing formula for the purposes of determining the weighting method optimal for the data.   

#### 2a. Create simplified balancing formulas
First, create shorter, simplified balancing formulas at each exposure time point.  
```{r}

#optional list of concurrent confounder
concur_conf <- "B18Raw.15"
concur_conf <-  NULL #empirical example 

#optional list of tv confounders to always retain (lag t-1)
keep_conf <- "InRatioCor.6"
keep_conf  <-  NULL #empirical example 

#optional custom formulas
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("short_form-6" = as.formula("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_assist"),
               "short_form-15" = as.formula("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_assist")
)
custom <- NULL #empirical example 


#required
type <- "short" 

#all inputs
short_formulas <- createFormulas(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, #required
                                 type = type, ti_confounders = ti_confounders, tv_confounders = tv_confounders, #required
                                 concur_conf = concur_conf, keep_conf = keep_conf, custom = custom, #optional
                                 home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


#### 2b. Create IPTW balancing weights using multiple weighting methods
We recommend users use the short formulas to create IPTW weights using all the available weighting methods: "glm", "gbm", "bart", "super", and "cbps".   
```{r}
formulas <- short_formulas

method <- "cbps"
weights.cbps <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                              method = method, read_in_from_file = FALSE,  #optional
                              # ints = TRUE,  #trying out optional weightitMSM inputs
                              home_dir = home_dir, verbose = TRUE, save.out = TRUE)  #optional

method <- "glm"
weights.glm <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                             method = method, read_in_from_file = FALSE, #optional
                             home_dir = home_dir, verbose = TRUE, save.out = TRUE)  #optional

method <- "gbm"
weights.gbm <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                             method = method, read_in_from_file = FALSE,  #optional
                             home_dir = home_dir, verbose = TRUE, save.out = TRUE)  #optional

method <- "bart"
weights.bart <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                              method = method, read_in_from_file = FALSE, #optional
                              home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

method <- "super"
weights.super <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                               method = method, read_in_from_file = FALSE, #optional
                               home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


#### 2c. Assess all weighting methods to determien optimal method
The next step is to assess balance for each weighting method using the short formulas to determine the optimal weighting method.  
```{r}
#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) #empirical example 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58", "PmEd2") #empirical example 

#required
type <- "weighted"
formulas <- short_formulas

weights <- weights.cbps 
balance_stats.cbps <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                    outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                    balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                    home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

weights <- weights.glm
balance_stats.glm <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                   outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                   balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                   home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
weights <- weights.gbm
balance_stats.gbm <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                   outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                   balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                   home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

weights <- weights.bart
balance_stats.bart <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                    outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                    balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                    home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

weights <- weights.super 
balance_stats.super <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                     outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                     balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                     home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


### STEP 3: Create Updated Formulas & Re-Specify Weights Using Optimal Weighting Method
The next step is to update the short balancing formulas with any imbalanced confounders and re-specify the IPTW weights.  

#### 3a. Assess balance with full balancing formulas
First, assess how well each of the IPTW achieve balance for all measured confounders using the full balancing formula.  
```{r}
#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) #empirical example 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58", "PmEd2") #empirical example 

#required
type <- "weighted"
formulas <- full_formulas

weights <- weights.cbps

#all inputs
balance_stats <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                               outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                               balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                               home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


#### 3b. Update simplified formulas 
Next, update the short formulas with any imbalanced confounders.  
```{r}
#optional custom formulas
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("update_form-6" = as.formula("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_assist"),
               "update_form-15" = as.formula("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_assist")
)
custom <- NULL #empirical example 

#optional list of concurrent confounder
concur_conf <- "B18Raw.15"
concur_conf <-  NULL #empirical example 

#optional list of tv confounders to always retain (lag t-1)
keep_conf <- "InRatioCor.6"
keep_conf <- NULL #empirical example 


#required
type <- "update"
bal_stats <- balance_stats

#all inputs
updated_formulas <- createFormulas(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, #required
                                   type = type, ti_confounders = ti_confounders, tv_confounders = tv_confounders, bal_stats = bal_stats, #required
                                   concur_conf = concur_conf, keep_conf = keep_conf, #optional
                                   home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


#### 3c. Create final balancing weights
Next, create final balancing weights using the optimal weighting method and updated balancing formulas.  
```{r}
#required
formulas <- updated_formulas

method <- "cbps" #list optimal weigthing method 

#all inputs
final_weights <- createWeights(data = data, exposure = exposure, outcome = outcome, formulas = formulas, #required
                               method = method, read_in_from_file = FALSE, #optional
                               home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


#### 3d. Trim final balancing weights
Next, trim the final balancing weights to reduce the heavy right tail.  

##### Main
First, trim the main weights.  
```{r}
# optional quantile of weights above which weights are trimmed (default is 0.95)
quantile <- NA 
quantile <- 0.95 #empirical example 

#required
weights <- final_weights

#all inputs
trim_weights <- trimWeights(exposure = exposure, outcome = outcome, weights = weights, #required
                            quantile = quantile, #optional
                            home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```

##### Sensitvity analyses
Next, conduct sensitivity analyses using two other quantile values.  
```{r}
quantile <- 0.92 #optional quantile of weights above which weights are trimmed (default is 0.95)
trim_weights.s1 <- trimWeights(exposure = exposure, outcome = outcome, weights = weights, #required
                               quantile = quantile, #optional
                               home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

quantile <- 0.98 #optional quantile of weights above which weights are trimmed (default is 0.95)
trim_weights.s2 <- trimWeights(exposure = exposure, outcome = outcome, weights = weights, #required
                               quantile = quantile, #optional
                               home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


### STEP 4: Conduct Final Balance Assessment
Next, conduct a final balance assessment using all measured confounders (i.e., the full balancing formulas).  

#### Main
First, conduct the main balance assessment.  
```{r}

#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1)  #empirical example 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58", "PmEd2") #empirical example 

#required
type <- "weighted"
formulas <- full_formulas
weights <- trim_weights

#all input
final_balance_stats <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                     outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                     balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                     home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional


#manually list remaining imbalanced covariates that are time-invariant or time-varying at t=1 for use in Step 5
covariates <- c("ESETA1.6", "InRatioCor.6", "gov_assist", "PMEd2") 

```


#### Sensitvity analyses
Next, conduct the recommended specifying sensitivity analyses to match the main analyses above.  

Note: If `save.out = TRUE`, please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:  

In the balance -> weighted folder, .csv/.html tables of:  
- balance statistics
- imbalanced statistics
- overall balance summary

In the balance -> weighted -> plots folder, .jpeg images of:
- summary balance plots

```{r}

#sensitivity tests
weights <- trim_weights.s1
final_balance_stats.s1 <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                        outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                        balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                        home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

weights <- trim_weights.s2
final_balance_stats.s2 <- assessBalance(data = data, exposure = exposure, exposure_time_pts = exposure_time_pts, #required
                                        outcome = outcome, type = type, formulas = formulas, weights = weights, #required
                                        balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                        home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


## PHASE 2: Assess Substantive Associations between Exposure and Outcome
Lastly, having attenuated confounder associations, we model substantive associations.  

### STEP 5: Fit Marginal Structural Model & Summarize  & Visualize Results

#### 5a. Select and fit a marginal outcome model
First, select and fit a marginal outcome model.  

##### Main
First, fit the main model.  
```{r}

#optional family/link information for glm
family <- NULL #empirical example
family <- gaussian

link <- NA  #empirical example
link <- "identity" 

# max interaction order (required for interaction models m2-3)
int_order <- NA
int_order <- 2

#covariates (required for covariate models m1, m3)
covariates <- NULL
covariates <- c("ESETA1.6", "gov_assist", "B18Raw.6",  "gov_assist:B18Raw.6", "ESETA1.6:B18Raw.6") 


#optional specification of epochs
epochs <- NULL
epochs <- data.frame(epochs = c("Infancy", "Toddlerhood", "Childhood"),  #empirical example
                     values = I(list(c(6, 15), c(24, 35), c(58)))) 

#required
weights <- trim_weights

# all inputs
#required
model <- "m1"
models <- fitModel(data = data, weights = weights, exposure = exposure, #required
                   exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                   family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                   home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


##### Sensitvity analyses
Next, fit the recommended  specifying sensitivity analyses to match the main analyses above.   

Note: If `save.out = TRUE`, please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:

in the model folder:
- .rds model file
- .docx table of model evidence

```{r}

weights <- trim_weights.s1
models.s1 <- fitModel(data = data, weights = weights, exposure = exposure, #required
                      exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                      family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                      home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

weights <- trim_weights.s2
models.s2 <- fitModel(data = data, weights = weights, exposure = exposure, #required
                      exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                      family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                      home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional
```


#### 5b. Estimate, compare, & visualize model-predicted outcome as a function of history
Lastly, estimate and compare user-specified exposure histories.   

##### Main
```{r}

#optional specification of epochs; only specify this if you did in model fitting (5a) --must be identical
epochs <- NULL
epochs <- data.frame(epochs = c("Infancy", "Toddlerhood", "Childhood"), 
                     values = I(list(c(6, 15), c(24, 35), c(58)))) #empirical example 

#optional list of quantiles specifying high and low cutoff values for continuous exposures; 
hi_lo_cut <- NULL
hi_lo_cut <- c(0.6, 0.3) #empirical example final choice

#optional reference history (required if comparisons are specified)
reference <- NULL
reference <- "l-l-l" #empirical example final choice
reference <- c("l-l-l", "l-l-h") #multiple

#optional comparison history/histories (required if reference specified)
comparison <- NULL 
comparison <- "h-h-h" #single
comparison <- c("h-h-h", "h-h-l") #multiple
comparison <- c("h-h-h", "h-l-l", "l-l-h", "h-h-l", "l-h-h") #empirical example final choice


#optional multiple comparion method; default is Benjamini-Hochburg, ("holm", "hochberg","hommel", "bonferroni", "BH", "BY", "fdr", "none" (see stats::p.adjust() documentation)
mc_comp_method <- NA
mc_comp_method <- "BH" #empirical example 

#optional specification of dose level (high or low) for dose count (default is "h")
dose_level <- NA
dose_level <- "h" #empirical example 

#optional exposure label for plotting
exp_lab <- NA
exp_lab <- "Economic Strain" #empirical example 

#optional outcome label for plotting 
out_lab <- NA
out_lab <- "Behavior Problems" #empirical example 

# optional list of colors (equal to number of epochs +1) or brewer palette for plotting #(see RColorBrewer::display.brewer.all() or https://r-graph-gallery.com/38-rcolorbrewers-palettes.html) for plotting default is 'Dark2'); 
colors <- NULL
colors <- c("Dark2") #empirical example
colors <- c("blue4", "darkgreen", "darkgoldenrod", "red2") #list number-of-exposure-main-effects-+1 colors

#required
model <- models #output from fitModel

#all inputs
results <- compareHistories(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                            epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                            mc_comp_method = mc_comp_method, dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                            home_dir = home_dir, verbose = TRUE, save.out = TRUE) #optional

```


##### Sensitvity analyses
We recommend specifying sensitivity analyses to match the main analyses above. 

Note: Please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:

in the histories folder, .html tables of:
- estimated mean outcome values for each hisotry
- history comparisons

in the plots folder, .jpeg images of:
- predicted outcomes values for each history

```{r}

model <- models.s1 
results.s1 <- compareHistories(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                               epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                               mc_comp_method = mc_comp_method, dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                               home_dir = home_dir, verbose = FALSE, save.out = TRUE) #optional

model <- models.s2 
results.s2 <- compareHistories(exposure = exposure, exposure_time_pts = exposure_time_pts, outcome = outcome, model = model, #required
                               epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                               mc_comp_method = mc_comp_method, dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                               home_dir = home_dir, verbose = FALSE, save.out = TRUE) #optional

```


## Package citations :) 
We are grateful to the authors of many existing packages that devMSMs draws from!

```{r}

grateful::cite_packages(out.dir = home_dir, omit = c("devMSMs", "devMSMsHelpers"), 
                        out.format = "docx")

```

