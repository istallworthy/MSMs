---
title: "devMSMsTutorialWorkflow"
author: "Isabella C. Stallworthy", "Meriah L. DeJoseph", "Emily R. Padrutt", "Noah Greifer", "Daniel Berry"
date: "`r Sys.Date()`"
output: html_document
---

Recommended workflow for using the devMSMs package with longitudinal data.

Step-by-step guidance to this workflow in the Supplement: 
https://docs.google.com/document/d/1WQGtfYAF4e6mREl5_fSgvmSG7nBzvT6w_iVQgROeAno/edit?pli=1, starting around page 11. 

Goals of this round of testing:
- make sure functions run with your data using both minimal and optional inputs
- assess the user output in the console and the items saved out from each function
- make sure the Supplement provides sufficient detail for running this .rmd file

The code is set up to identify all possible inputs to each function (required and optional) to aid the user's use of the full range of package functionality. Example possible values for the optional input are shown for each function, including a NULL/NA option if the user does not wish to specify an optional input. Alternatively, the user could modify the call to the function and remove the optional input argument entirely. 

Some notes on functions in R:
For the required arguments  (e.g., arg1, arg2) in a function, you can supply them directly to the function as variables in your global environment.
arg2 = 9.5
E.g., function(arg1, arg2).

For optional arguments (e.g., arg3) in a function, you should assign them with an = to indicate to the function that they are being specified. 
arg3 = 4.5
E.g., function(arg1, arg2, arg3 = arg3)
or 
E.g., function(arg1, arg2, arg3 = 4.5)


Please use the Supplement and/or type ?functionName into the console for more guidance on the arguments for each function. These two sources should match but let me know if you see discrepancies. 

PLEASE Slack me right away with any issues or confusion you encounter --there will likely be bumps in the road as we test this!
Feel free to put feedback in your version of this .rmd, in the notes Google doc, or in the Supplement as comments. 


## Getting started
Until devMSMs is available on CRAN, you will need to install it directly from Github. 
You will always need to install the devMSMs helper functions from Github (deveMSMsHelpers) if you wish to use them. 

```{r}

#first, install devtools
install.packages("devtools")
library(devtools)

#you may need to do this if you are having issues installing devMSMs --will talk to NG about it
# install.packages("SuperLearner")

#install devMSMs from github
install_github("istallworthy/devMSMs")
library(devMSMs)

#install helper files from github. Note: you need to install devMSMs for devMSMsHelpers::inspectData() to work. 
install_github("istallworthy/devMSMsHelpers")
library(devMSMsHelpers)

#note: if I update Github to fix something, you may need to first uninstall the package(s) by running the following code:
# remove.packages("devMSMs") or 
# remove.packages("devMSMsHelpers")
#prior to re-installing using the code above. Sorry, this is annoying! There may also be a short lag between when I update something on Github and when it becomes available for install. 

#conducting package checks & tests from documentation --just for IS to run; comment out when testing the workflow
#devtools::check()
```


## Specifying Required Package Core Inputs
The user should change all fields in this code chunk to match their home directory and wide data.

```{r}

#set seed for reproducibility 
set.seed(1234)

#required if you wish to use save.out = TRUE in the functions
home_dir <- '/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/isa' #note: no / after

#required
exposure <- "ESETA1"

#required
exposure_time_pts <- c(6, 15, 24, 35, 58)

#required
outcome <- "StrDif_Tot.58"

#optional; list in wide format
tv_confounders <- c("SAAmylase.6","SAAmylase.15", "SAAmylase.24",
                    "MDI.6", "MDI.15",                                            
                    "RHasSO.6", "RHasSO.15", "RHasSO.24","RHasSO.35", "RHasSO.58",                                         
                    "WndNbrhood.6","WndNbrhood.24", "WndNbrhood.35", "WndNbrhood.58",                                       
                    "IBRAttn.6", "IBRAttn.15", "IBRAttn.24",                                   
                    "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58",                                           
                    "HOMEETA1.6", "HOMEETA1.15", "HOMEETA1.24", "HOMEETA1.35", "HOMEETA1.58",                               
                    "InRatioCor.6", "InRatioCor.15", "InRatioCor.24", "InRatioCor.35", "InRatioCor.58",                                 
                    "ESETA1.6", "ESETA1.15", "ESETA1.24", "ESETA1.35", "ESETA1.58",                                         
                    "CORTB.6", "CORTB.15", "CORTB.24",                                                                                
                    "EARS_TJo.24", "EARS_TJo.35",                                        
                    "LESMnPos.24", "LESMnPos.35",                                  
                    "LESMnNeg.24", "LESMnNeg.35",       
                    "ALI_Le.35",
                    "StrDif_Tot.35", "StrDif_Tot.58",    
                    # "EF_avg_perc.35", "EF_avg_perc.58"
                    "fscore.35", "fscore.58"
) 

#required
ti_confounders <- c("state", "BioDadInHH2", "PmAge2", "PmBlac2", "TcBlac2", "PmMrSt2", "PmEd2", "KFASTScr",
                    "RMomAgeU", "RHealth", "HomeOwnd", "SWghtLB", "SurpPreg", "SmokTotl", "DrnkFreq",
                    "peri_health", 
                    # "mat_health", "gov_asst")
                    "caregiv_health", "gov_assist")

```


## STEP P: Preliminary Steps for Reading in, Formatting, & Inspecting Data
Choose from the following preliminary steps with the goal of assigning to 'data' one of the following for use in the package:
- a single data frame of data in wide format with no missing data 
- a mids object (output from mice::mice()) of data imputed in wide format
- a list of data imputed in wide format as data frames

Data columns should be either numeric or factor form and the ID column should be numeric.


### P1. Format Data
#### P1a. Format single data frame of long data 
These data can be complete or have missingness. 

```{r}

#reading a long data file here for use in subsequent steps
# data_long <- read.csv("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/Tutorial paper/merged_tutorial_filtered.csv", #add path to your long data file here if you want to begin with long data
#                       header = TRUE)

# if you have long data with wrong time, id, and/or missing indicators and need to format
data_long_f <- formatLongData(home_dir, data_long, exposure, exposure_time_pts, outcome, 
                              time_var = "TAge", #list original time variable here if it's not "WAVE"
                              id_var = "S_ID", #list original id variable here if it's not "ID"
                              missing = NA, #list missing value here
                              factor_confounders = c("state","TcBlac2", "PmBlac2", "RHasSO", "PmMrSt2", 
                                                     "HomeOwnd", "SurpPreg", "BioDadInHH2"), #list factor variables here
                              save.out = TRUE) 

```


#### P1b. Convert single long data frame to wide format

```{r}

v <- sapply(strsplit(tv_confounders, "\\."), "[", 1)
v <- v[!duplicated(v)]
data_wide <- stats::reshape(data = data_long_f, 
                            idvar = "ID", #list ID variable in your dataset
                            v.names = v, 
                            timevar = "WAVE", # list time variable in your long dataset
                            times = c(6, 15, 24, 35, 58), # list all time points in your dataset
                            direction = "wide")
data_wide <- data_wide[, colSums(is.na(data_wide)) < nrow(data_wide)]



# read in formatted wide data
data_wide <- read.csv("/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/Tutorial paper/merged_tutorial_filtered.csv", #add path to your wide, formatted data file here 
                      header = TRUE)

#make factor variables factor 
factor_covars <- c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                   "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                   "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58")
data_wide[, factor_covars] <- as.data.frame(lapply(data_wide[, factor_covars], as.factor))



#if you want to test out median split binary exposure
library(dplyr)
e <- data_wide[, paste(exposure, exposure_time_pts, sep = ".")]
m <- sapply(e, median)

data_wide[, paste(exposure, exposure_time_pts, sep = ".")] <- 
  as.data.frame(lapply(seq(ncol(e)), function(x) {
  as.factor(ifelse(e[, x] > m[x], 1, 0))
}))


#if you have no missing data and want to use the package with a single wide data frame
data <- data_wide

```


### P2. Impute Data to Account for Missingness
#### P2a. Multiply impute single wide, formatted data frame using mice

```{r}

#optional
m <- NA
m <- 1 #number of imputations (default is 5)

#optional
method <- NA
method <- "rf" # provide an imputation method pmm, midastouch, sample, cart , rf (default)

#optional
maxit <- NA
maxit <- 1 #maximum iterations for imputation (default is 5)

#optional
#please see the mice::mice() documentation for more optimal input to customize mice(): https://www.rdocumentation.org/packages/mice/versions/3.16.0/topics/mice


#note: right now (just for testing purposes), the function is set up for maxit = 0 to go fast, default is 5
imputed_data <- imputeData(data_wide, home_dir, exposure, outcome, 
                           m = m, method = method, maxit = maxit, para_proc = FALSE, read_imps_from_file = "no", 
                           save.out = TRUE)


#if you want to use the package with imputed data
data <- imputed_data



#OR, read in a saved mids object
imputed_data <- readRDS(paste0(home_dir, 
                               "/imputed_data.rds")) #place your .rds file in your home directory and change the name of file here

#if you want to use the package with imputed data
data <- imputed_data



#optional: extract first imputed dataset for testing
library(mice)
data <- mice::complete(imputed_data, 1) #just for testing purposes

#make factors
factor_covars <- c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                   "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                   "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58")
data[, factor_covars] <- as.data.frame(lapply(data[, factor_covars], as.factor))


```


#### P2b. Read in wide imputed data saved locally as .csvs

```{r}

#read in imputed csv files to list
folder <- "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/MSMs/testing/isa/imputations/" #change this to match your local folder
files <- list.files(folder, full.names = TRUE, pattern = "\\.csv")


#if you want to use the package with a list of imputed data from above
data <- lapply(files, function(file) {
  imp_data <- read.csv(file)
  imp_data
})

#make factors
factor_covars <- c("state", "TcBlac2","BioDadInHH2","HomeOwnd", "PmBlac2",       
                   "PmMrSt2", "SurpPreg", "RHealth", "SmokTotl", "DrnkFreq",
                   "RHasSO.6", "RHasSO.15", "RHasSO.24", "RHasSO.35", "RHasSO.58")
data <- lapply(data, function(x){
  x[, factor_covars] <- as.data.frame(lapply(x[, factor_covars], as.factor))
  x
})

```


### P3. Optional: Identify Exposure Epochs
If you specify exposure epochs, we recommend doing so consistently throughout this workflow.

```{r}

#change this to match your data/theory 
epochs <- data.frame(epochs = c("Infancy", #list user-specified names
                                "Toddlerhood", 
                                "Childhood"), 
                     values = I(list(c(6, 15), #list corresponding time points from data
                                     c(24, 35), 
                                     c(58)
                     ))) 

```


### P4. Recommended: Specify and Inspect Exposure Histories
#### P4a. Create cutoff values for continuous exposures
If you specify high and low cutoffs, we recommend doing so consistently throughout this workflow. 

```{r}

#optional list of quantiles specifying high and low cutoff values, respectively, for continuous exposures; default is median
hi_lo_cut <- c(0.6, 0.3)

```


#### P4b. Specify hypotheses-relevant exposure histories
If you specify reference and comparison histories, please do so consistently throughout this workflow. 

```{r}

#optional reference history (required if comparisons are specified)
reference <- NULL
reference <- "l-l-l"
reference <- c("l-l-l", "l-l-h")


#optional comparison history/histories
comparison <- NULL 
comparison <- "h-h-h" #single
comparison <- c("h-h-h", "h-h-l") #multiple

```


#### P4c. Inspect exposure histories (recommended)

```{r}

#all inputs
inspectData(data, home_dir, exposure, exposure_time_pts, outcome, ti_confounders, # required input
            tv_confounders = tv_confounders, epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional input
            verbose = TRUE, save.out = TRUE) #optional input


# #or some optional inputs
# inspectData(data, home_dir, exposure, exposure_time_pts, outcome, tv_confounders, ti_confounders,
#             hi_lo_cut = hi_lo_cut) #assign w/ = any subset of optional inputs
# 
# 
# #or minimum inputs
# inspectData(data, home_dir, exposure, exposure_time_pts, outcome, 
#             ti_confounders, tv_confounders = tv_confounders)

```



## PHASE 1: Balance Confounders

### STEP 1: Pre-Balance Checking
#### 1a. Create full formulas
If you specify concurrent confounders to retain, we recommend doing so consistently throughout this workflow. 

```{r}

#optional concurrent confounders
concur_conf <- NULL 
concur_conf <- "B18Raw.15"


#optional custom formulas
custom <- NULL
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("full_form-6" = as.formula ("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_asst"),
               "full_form-15" = as.formula ("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_asst")
)



#required
type <- "full"

#all inputs
full_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, #required
                                tv_confounders = tv_confounders, concur_conf = concur_conf, custom = custom, #optional
                                verbose = TRUE, save.out = TRUE) #optional

# #or minimum inputs
# full_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, tv_confounders = tv_confounders)


```

#### 1b. Exploratory prebalance assessment
If you specify balance threshold(s), we recommend doing so consistently throughout this workflow. 

```{r}

#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("PmAge2", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58") 

#required
type <- "prebalance"
formulas <- full_formulas

#all inputs
prebalance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, #required
                                  balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                  verbose = TRUE, save.out = TRUE) #optional

# # #or minimum inputs
# prebalance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas)

```


### STEP 2: Determine optimal weighting method
#### 2a. Create simplified balancing formulas
If you specify concurrent confounders to retain and/or confounders to keep in balancing formulas, we recommend doing so consistently throughout this workflow. 
```{r}

#optional list of concurrent confounder
concur_conf <-  NULL 
concur_conf <- "B18Raw.15"

#optional list of tv confounders to always retain (lag t-1)
keep_conf  <-  NULL 
keep_conf <- "InRatioCor.6"

#optional custom formulas
custom <- NULL
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("short_form-6" = as.formula ("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_asst"),
               "short_form-15" = as.formula ("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_asst")
)


#required
type <- "short" 

#all inputs
short_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, #required
                                 tv_confounders = tv_confounders, concur_conf = concur_conf, keep_conf = keep_conf, custom = custom, #optional
                                 verbose = TRUE, save.out = TRUE) #optional

# #or minimum inputs
# short_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, tv_confounders = tv_confounders)

```

#### 2b. Create IPTW balancing weights using multiple weighting methods

```{r}

## Create balancing weights with simplified formulas

#required
formulas <- short_formulas

# optional weighting method "ps", "glm", "gbm", "bart", "super", "cbps"  (default is cbps)
method <- "ps"

# other optional input for each weighting method
# We suggest reviewing the WeightIt documentation (https://ngreifer.github.io/WeightIt/reference/weightitMSM.html) for more information about the additional optional arguments available for each of the weighting methods.


#all input
weights.ps <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                            method = method, read_in_from_file = "no", #optional
                            verbose = TRUE, save.out = TRUE) #optional 

# #or minimum input
# weights.ps <- createWeights(home_dir, data, exposure, outcome, formulas)



method <- "cbps"
weights.cbps <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                              method = method, read_in_from_file = "no",  #optional
                              verbose = TRUE, save.out = TRUE)  #optional

method <- "glm"
weights.glm <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                             method = method, read_in_from_file = "no",  #optional
                             verbose = TRUE, save.out = TRUE)  #optional

method <- "gbm"
weights.gbm <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                             method = method, read_in_from_file = "no",  #optional
                             verbose = TRUE, save.out = TRUE)  #optional
# testing additional arguments
weights.gbm <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                             method = method, read_in_from_file = "no", #optional
                             verbose = TRUE, save.out = TRUE, #optional
                             criterion = "p.max", distribution = "laplace") ##optional; can now add any input that weightitMSM() takes for "gbm

method <- "bart"
weights.bart <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                              method = method, read_in_from_file = "no", #optional
                              verbose = TRUE, save.out = TRUE) #optional

method <- "super"
weights.super <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                               method = method, read_in_from_file = "no", #optional
                               verbose = TRUE, save.out = TRUE) #optional
#testing library specification
weights.super <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                               method = method, read_in_from_file = "no", #optional
                               verbose = TRUE, save.out = TRUE,
                               SL.library = "SL.mean") #can now specify SuperLearner library (default is "SL.glm")

```


#### 2c. Assess all weighting methods
If you specify balance threshold(s), we recommend doing so consistently throughout this workflow. 

```{r}

#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("PmAge2", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58") 


#required
type <- "weighted"
formulas <- short_formulas

weights <- weights.ps 


#all inputs
balance_stats.ps <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                  balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                  verbose = TRUE, save.out = TRUE) #optional

# #minimum inputs
# balance_stats.ps <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights)


weights <- weights.cbps 
balance_stats.cbps <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                    balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                    verbose = TRUE, save.out = TRUE) #optional

weights <- weights.glm 
balance_stats.glm <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                   
                                   balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                   verbose = TRUE, save.out = TRUE) #optional
weights <- weights.gbm
balance_stats.gbm <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                   balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                   verbose = TRUE, save.out = TRUE) #optional

weights <- weights.bart
balance_stats.bart <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                    balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                    verbose = TRUE, save.out = TRUE) #optional

weights <- weights.super 
balance_stats.super <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                     balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                     verbose = TRUE, save.out = TRUE) #optional

```


### STEP 3: Re-specify weights using optimal weighting method
#### 3a. Assess balance using full formulas
If you specify balance threshold(s), we recommend doing so consistently throughout this workflow. 

```{r}

#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("PmAge2", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58") 


#required
type <- "weighted"
formulas <- full_formulas

weights <- weights.ps

#all inputs
balance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                               balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                               verbose = TRUE, save.out = TRUE) #optional

#minimum inputs
# balance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights)

```


#### 3b. Update short formulas 
If you specify concurrent confounders to retain and/or confounders to keep in balancing formulas, we recommend doing so consistently throughout this workflow. 
```{r}

#optional custom formulas
custom <- NULL
#note: below is an example of just two of the entries in the list of a custom formula --you would need to make a list that contains entries for each exposure time point that mimics the output of createFormulas. <--Run createFormulas to see what it should look like.
custom <- list("update_form-6" = as.formula ("ESETA1.6 ~ BioDadInHH2 + DrnkFreq + gov_asst"),
               "update_form-15" = as.formula ("ESETA1.15 ~ BioDadInHH2 + DrnkFreq + gov_asst")
)


#optional list of concurrent confounder
concur_conf <-  NULL 
concur_conf <- "B18Raw.15"


#optional list of tv confounders to always retain (lag t-1)
keep_conf <- NULL 
keep_conf <- "InRatioCor.6"


#required
type <- "update"
bal_stats <- balance_stats

#all inputs
updated_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, bal_stats = bal_stats, #required
                                   tv_confounders = tv_confounders, concur_conf = concur_conf, keep_conf = keep_conf, #optional
                                   verbose = TRUE, save.out = TRUE) #optional

#minimum inputs
# updated_formulas <- createFormulas(home_dir, exposure, exposure_time_pts, outcome, type, ti_confounders, tv_confounders = tv_confounders,
#                                    bal_stats = bal_stats)

```

#### 3c. Re-create weights

```{r}

#required
formulas <- updated_formulas

method <- "ps"

#all inputs
final_weights <- createWeights(home_dir, data, exposure, outcome, formulas, #required
                               method = method, read_in_from_file = "no", #optional
                               verbose = TRUE, save.out = TRUE) #optional

#minimum inputs
# final_weights <- createWeights(home_dir, data, exposure, outcome, formulas)

```


#### 3d. Truncate weights
##### Main

```{r}

# optional quantile of weights above which weights are trimmed (default is 0.95)
quantile <- NA 
quantile <- 0.95 

#required
weights <- final_weights


#all inputs
trim_weights <- trimWeights(home_dir, exposure, outcome, weights, #required
                            quantile = quantile, #optional
                            verbose = TRUE, save.out = TRUE) #optional

# #minimum inputs
# trim_weights <- trimWeights(home_dir, exposure, outcome, weights)

```

##### Sensitvity analyses

```{r}

#sensitivity tests   
quantile <- 0.92 #optional quantile of weights above which weights are trimmed (default is 0.95)
trim_weights.s1 <- trimWeights(home_dir, exposure, outcome, weights, #required
                               quantile = quantile, #optional
                               verbose = TRUE, save.out = TRUE) #optional

quantile <- 0.98 #optional quantile of weights above which weights are trimmed (default is 0.95)
trim_weights.s2 <- trimWeights(home_dir, exposure, outcome, weights, #required
                               quantile = quantile, #optional
                               verbose = TRUE, save.out = TRUE) #optional

```


### STEP 4: Final Balance Assessment
#### Main
If you specify balance threshold(s), we recommend doing so consistently throughout this workflow. 

```{r}

#optional balance threshold specification
balance_thresh <- NULL
balance_thresh <- 0.1 
balance_thresh <- c(0.05, 0.1) 

#optional list of important confounders
imp_conf <- NULL
imp_conf <- c("PmAge2", "B18Raw.6", "B18Raw.15", "B18Raw.24", "B18Raw.58") 

#required
type <- "weighted"
formulas <- full_formulas
weights <- trim_weights


#all input
final_balance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                     balance_thresh = balance_thresh, imp_conf = imp_conf, #optional
                                     verbose = TRUE, save.out = TRUE) #optional

# #minimum input
# final_balance_stats <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights)


#manually list remaining imbalanced covariates that are time-invariant or time-varying at t=1 for use in Step 5
imbalanced_covars <- c("ESETA1.6", "gov_asst", "B18Raw.6") 

```


#### Sensitvity analyses
We recommend specifying sensitivity analyses to match the main analyses above. 
Note: Please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:

In the balance -> weighted folder, .csv/.html tables of:
- balance statistics
- imbalanced statistics
- overall balance summary

In the balance -> weighted -> plots folder, .jpeg images of:
- summary balance plots

```{r}

#sensitivity tests
weights <- trim_weights.s1
final_balance_stats.s1 <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                        balance_thresh = balane_thresh, imp_conf = imp_conf, #optional
                                        verbose = TRUE, save.out = TRUE) #optional

weights <- trim_weights.s2
final_balance_stats.s2 <- assessBalance(home_dir, data, exposure, exposure_time_pts, outcome, type, formulas, weights, #required
                                        balance_thresh = balane_thresh, imp_conf = imp_conf, #optional
                                        verbose = TRUE, save.out = TRUE) #optional
```


## PHASE 2: Assess Substantive Associations between Exposure and Outcome

### STEP 5: Fit marginal structural model & summarize results
#### 5a. Fit marginal structural model
##### Main
If you specify exposure epochs, we recommend doing so consistently throughout this workflow. 

```{r}

#optional family/link information for glm
family <- NULL
family <- gaussian

link <- NA
link <- "identity" 

# max interaction order (required for interaction models m2-3)
int_order <- NA
int_order <- 3

#covariates (required for covariate models m1, m3)
covariates <- NULL
covariates <- imbalanced_covariates 
covariates <- c("ESETA1.6", "gov_assist", "B18Raw.6") 


#optional specification of epochs
epochs <- NULL
epochs <- data.frame(epochs = c("Infancy", "Toddlerhood", "Childhood"), 
                     values = I(list(c(6, 15), c(24, 35), c(58)))) 


#required
weights <- trim_weights


# all inputs
#required
model <- "m3"
models <- fitModel(home_dir, data, weights, exposure, exposure_time_pts, outcome, model, #required
                   family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                   verbose = TRUE, save.out = TRUE) #optional

#minimum inputs
# model <- "m0"
# models <- fitModel(home_dir, data, weights, exposure, exposure_time_pts, outcome, model, 
#                    epochs = epochs)

# #some optional inputs
# models <- fitModel(home_dir, data, weights, exposure, exposure_time_pts, outcome, model,
#                    int_order = int_order)

```


##### Sensitvity analyses
We recommend specifying sensitivity analyses to match the main analyses above. 
Note: We recommend specifying sensitivity analyses to match the main analyses above. 
Note: Please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:

in the model folder:
- .rds model file
- .docx table of model evidence

```{r}

#sensitivity tests
weights <- trim_weights.s1
models.s1 <- fitModel(home_dir, data, weights, exposure, exposure_time_pts, outcome, model, #required
                      family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                      verbose = TRUE, save.out = TRUE) #optional

weights <- trim_weights.s2
models.s2 <- fitModel(home_dir, data, weights, exposure, exposure_time_pts, outcome, model, #required
                      family = family, link = link, int_order = int_order, covariates = covariates, epochs = epochs, #optional
                      verbose = TRUE, save.out = TRUE) #optional
```


#### 5b. Estimate, compare, & visualize histories
##### Main
If you specify exposure epochs, we recommend doing so consistently throughout this workflow. 

```{r}

#optional specification of epochs; only specify this if you did in model fitting (5a) --must be identical
epochs <- NULL
epochs <- data.frame(epochs = c("Infancy", "Toddlerhood", "Childhood"), 
                     values = I(list(c(6, 15), c(24, 35), c(58)))) 

#optional list of quantiles specifying high and low cutoff values for continuous exposures; 
hi_lo_cut <- NULL
hi_lo_cut <- c(0.6, 0.3) #default are median split

#optional reference history (required if comparisons are specified)
reference <- NULL
reference <- "l-l-l"
reference <- c("l-l-l", "l-l-h") #multiple

#optional comparison history/histories (required if reference specified)
comparison <- NULL 
comparison <- "h-h-h" #single
comparison <- c("h-h-h", "h-h-l") #multiple

#optional multiple comparion method
mc_comp_method <- NA
mc_comp_method <- "bonferroni" # default is Benjamini-Hochburg, ("holm", "hochberg","hommel", "bonferroni", "BH", "BY", "fdr", "none" (see stats::p.adjust() documentation)

#optional specification of dose level (high or low) for dose count
dose_level <- NA
dose_level <- "h" #(default is "h")

#optional exposure label for plotting
exp_lab <- NA
exp_lab <- "Economic Strain" 

#optional outcome label for plotting 
out_lab <- NA
out_lab <- "Executive Functioning" 

# optional list of colors (equal to number of epochs +1) or brewer palette for plotting
colors <- NULL
colors <- c("Dark2")  #(see RColorBrewer::display.brewer.all() or https://r-graph-gallery.com/38-rcolorbrewers-palettes.html) for plotting default is 'Dark2'); 
colors <- c("blue4", "darkgreen", "darkgoldenrod", "red2") #list number-of-exposure-main-effects-+1 colors


#required
model <- models #output from fitModel


#all inputs
results <- compareHistories(home_dir, exposure, exposure_time_pts, outcome, model, #required
                            epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                            mc_comp_method = mc_comp_method, dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                            verbose = FALSE, save.out = TRUE) #optional


# #minimum inputs
# reference <- "l-l-l-l-l"
# comparison <- c("h-h-h-h-h", "l-h-h-h-h") #multiple
# results <- compareHistories(home_dir, exposure, exposure_time_pts, outcome, model, epochs = epochs)
# # reference = reference, comparison = comparison) # for our data, specify ref and comp bc too many comparisons for exposure time point; 

```


##### Sensitvity analyses
We recommend specifying sensitivity analyses to match the main analyses above. 
Note: Please run the above main analysis and then rename or relocate the following output, before running each sensitivity test and renaming/relocating each one in the same manner:

in the histories folder, .html tables of:
- estimated mean outcome values for each hisotry
- history comparisons

in the plots folder, .jpeg images of:
- predicted outcomes values for each history

```{r}

#sensitivity tests
model <- models.s1 
results.s1 <- compareHistories(home_dir, exposure, exposure_time_pts, outcome, model, #required
                               epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                               mc_comp_method = mc_method , dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                               verbose = FALSE, save.out = TRUE) #optional

model <- models.s2 
results.s2 <- compareHistories(home_dir, exposure, exposure_time_pts, outcome, model, #required
                               epochs = epochs, hi_lo_cut = hi_lo_cut, reference = reference, comparison = comparison, #optional
                               mc_comp_method = mc_method , dose_level = dose_level, exp_lab = exp_lab, out_lab = out_lab, colors = colors, #optional
                               verbose = FALSE, save.out = TRUE) #optional

```


## Package citations :) 
We are grateful to the authors of many existing packages that devMSMs draws from!

```{r}

grateful::cite_packages(out.dir = home_dir, omit = c("devMSMs", "devMSMsHelpers"),  out.format = "docx")

```

