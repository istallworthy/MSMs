---
title: "examplePipeline for the devMSMs package"
author: "Isa Stallworthy", "Meriah DeJospeh", "Emily Padrutt", "Daniel Berry"
date: 10/27/2022
output: html_document
---

This code provides a framework for testing the putative causal effects of exposure histories on outcome(s) that measured at a single final time point. 

The most updated version of this package can be found at: https://github.com/istallworthy/MSMs 

Example pipeline running the MSM functions in sequence based on causal questions centered on the Family Life Project (FLP). 


### Load package
```{r}
# install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
library(devtools)
library(roxygen2)
load_all()

devtools::document()

# traceback()
```

### 1. Provide user inputs to create the msm object
Please provide user input for the following fields to create an msm object that will be used in all future functions. 

```{r}
# source("") #string

object <- msmObject(
  
  ##### Directories Information (required) ####
  # directories to long dataset 
  # data_path = "~/Library/CloudStorage/Box-Box/BSL General/Meriah/TSST MSM PAPER/MSMs/!MSM package test_10.17.22/MSMs-main/MSM_ALL_LONG_8.25.20.csv", #string
  data_path ="/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/MSM_ALL_LONG_8.25.20.csv",
  
  # home_dir = "~/Library/CloudStorage/Box-Box/BSL General/Meriah/TSST MSM PAPER/MSMs/!MSM package test_10.17.22/MSMs-main", #string
  home_dir = "/Users/isabella/Library/CloudStorage/Box-Box/BSL General/Isa/MSMs/Meriah test/", #string
  
  
  #### Dataset Information (required) ####
  #individual identifier 
  ID = "s_id", #string
  
  #time points of interest and name of time variable in long dataset
  time_pts=c(6, 15, 24, 35, 58, 90, 154), #list of integers
  time_var="WAVE", #string
  
  #marker for missing data
  missing=-9999, #string or integer(s)
  
  #list out all variables that are time-varying; default is none
  time_varying_variables= c("AGE", "INR", "WIND", "PCX_POS", "PCX_NEG", "BSI", "TIMEofDAY", "CORTBASE", "sAABASE", "ESETA1", "HOMEETA1", "CTSETA1"),
  
  #list all continuous variables here; all others assumed ordinal for imputation; default is none 
  continuous_variables=c("GrosPay1", "RMomAgeU", "RMAge1st", "SwghtLB","RWghtLb", "IBQDnovm", "MDI", "AGE", "INR", "WIND", "PCX_POS", "PCX_NEG", "BSI", "TIMEofDAY", "CORTBASE", "sAABASE", "ESETA1", "HOMEETA1", "CTSETA1"),
  
  #list any covariates that are factors here; default is none, or that all variables are continuous 
  factor_covariates=c("NC","TcSex2", "TcBlac2"), #list of characters
  
  
  #### Imputation Specification (optional) ####
  #number of imputations for step 4; default is 5 --add any other imputation parameters here?
  m=2, #integer
  
  
  #### Exposure Information (required) ####
  #exposures of interest that potentially have causal effects on outcomes and exposure time point(s) (corresponding to values in time_var)
  exposures=c("HOMEETA1", "CTSETA1", "ESETA1"), #list of strings
  exposure_time_pts=c(6, 15, 24, 35, 58, 90, 154), #list of integers
  
  
  #### Balancing Information (optional) ###
  balance_thresh=0.12, #correlation value above which covariates are not considered balanced with respect to exposure/outcome;used for finding potential confounds and asessing balance; default set to 0.12 from Stuart et al.
  
  weights_percentile_cutoff=0.90, #percentile cutoff value for truncating weights to avoid heavy tails; default is 0.90; we recommend sensitivity analyses with several values
  
  
  #### History Comparison Information (required & optional) ####
   #developmental time periods (and their corresponding time points) that constitute regime/history units of interest --these should be meaningful periods of time that may collapse over exposure time points to result in more manageable combinations for exploring effects of exposure histories (e.g, chronically high exposure vs. exposure only in infancy) on outcomes
  exposure_epochs=data.frame(epochs=c("early", "middle", "late"), #user-created names of epochs as a list of strings
                             values=I(list(c(6,15), c(24,35), c(58, 90, 154)))), #time point(s) encompassed in each epoch that correspond to data as a list of numeric lists
  
    #inspect the following exposure levels denoting all possible histories of high ("h") and low ("l") exposure over the epochs
  # apply(gtools::permutations(2, nrow(object$exposure_epochs), c("l", "h"), repeats.allowed=TRUE), 1, paste, sep="", collapse="-")
  #select one of the above histories, or permutations of high ("h") and low ("l") levels of exposure (one leve for each of the exposure epochs) as your reference event, or the history/sequence to which you would like to compare the other histories  
  reference="l-l-l", #optional: set a reference history of high ("h") and low ("l") levels of exposure separate by a "-"; the default is set to the history denoting low ("l") exposure at all time points; character string of "h" and "l" separated by a "-" 
  comparisons="", #optional: set a comparison history from list above; the default is to include all non-reference histories as comparisons; character string of "h" and "l" separated by a "-" 
  
  hi_cutoff=.75, #optional integer value for quantile value constituting the threshold for "high" levels of an exposure; default set to 0.75
  lo_cutoff=.25, #optional integer value for quantile value constituting the threshold for "low" levels of an exposure; default set to 0.25

  mc_method="BH", #optional method for multiple comparison correction for linear hypothesis tests; default is Benjamini-Hochburg, options are "holm", "hochberg","hommel", "bonferroni", "BH", "BY", "fdr", "none" (see stats::p.adjust() documentation)
  
  
  #### Outcome Information (required) ####
  #outcomes of interest and outcome time point
  outcomes=c("CORTBASE", "sAABASE"), #list of strings 
  outcome_time_pt=154, #integer value
  
  
 #### Inclusion Information (optional) ####
  #list any variables for mandatory inclusion in balancing for theoretical or other reasons (regardless of their correlations with exposure/outcome); if it is a time-varying covariate the code will include it in the forms for all time points; default set to none
  mandatory_keep_covariates=c("INR"),
  
  
  #### Exclusion Information (optional) ####
  #list out any variables that should be excluded from consideration as confounds via balancing based on practical or theoretical reasons; default set to none
  exclude_covariates=c("Index1", "WAVE2", "WAVE3", "LIVEVID", "SUBSTRESS", "TANNERALL"), #list of characters
  
  #list any time-varying variables that may get imputed but should NOT be present in the dataset because of planned missingness design with ".time_pt" format
  time_var_exclude=c("PCX_POS.58", "PCX_NEG.58", "PCX_POS.90", "PCX_NEG.90", "PCX_POS.154", "PCX_NEG.154", "CORTBASE.58", "CORTBASE.90", "sAABASE.58","sAABASE.90", "TIMEofDAY.6", "TIMEofDAY.15", "TIMEofDAY.24", "TIMEofDAY.35", "TIMEofDAY.58", "TIMEofDAY.90","TIMEofDAY.15", "WIND.15", "WIND.24", "WIND.154", "BSI.35","BSI.90", "BSI.154"), #default set to none
  
  #when creating a balancing formula at each time point, we need to remove potential colliders at that time point. list potential colliders (i.e., variables that could cause both the treatment and the outcome) at each time point below. these will be EXCLUDED from balancing only at the time point of the exposure and outcome, and at lagged values if specified by the user, as balancing on colliders can lead to problematic outcomes; default is none
  potential_colliders=data.frame(exp_out_pair=c("HOMEETA1-CORTBASE", "ESETA1-CORTBASE"), #names of exposure-outcome pair separated by "-", no spaces; list of strings
                                 colliders=I(list(c("INR"), c("HOMEETA1", "AGE"))), #list of colliders for each exposure-outcome pair; list of lists of strings
                                 exclude_lags=c("T", "F")), #whether or not lagged values of listed colliders, if they are time-varying, should also be excluded ("Y"=exclude lagged values); one entry per exp-out pair; list of strings
                                 
 
 ### Plotting Information (optional) ###
 #optional list of alternative labels for exposures that will be used only for plotting
   exposure_labels=c("Home Resources", "Conflict in the Home", "Threat"), 
   
  #optional list of alternative labels for outcomes that will be used only for plotting
   outcome_labels=c("Cortisol","Salivary Alpha Amalayse"),
 
 #the plots color data by dose (i.e., number of epochs) of high exposure; optional list of colors (equal to number of epochs +1) or brewer palette (see RColorBrewer::display.brewer.all() or https://r-graph-gallery.com/38-rcolorbrewers-palettes.html) 
   colors=(c("Dark2")) #list of string(s); default is 'Dark2'); c("blue4", "darkgreen", "darkgoldenrod", "red2")
)

```


### 2. Format data & identify covariates
Reads in data and formats for analysis. The user has the option to specify any covariates that are factors. 
```{r}

#Creates the necessary directories in your home directory and formats your dataset
data <- formatDataStruct(object)

#Makes separate datasets for each time point 
time_pt_datasets <- makeTimePtDatasets(object, data)

```


### 3. Identify which covariates could potentially confound the relationship(s) between exposure(s) and outcome(s) that will be used to create balancing weights
MSMs focus on the covariates that are associated with both exposure(s) and outcome(s) and thus could act as a confounding variable. More specifically, potential confounds for a given exposure-outcome pair at a given time point include any covariate: correlated with the exposure at that time point, lagged values of the exposure, or correlated with the outcome. The method assumes that you have included all possible confounding variables as columns in your dataset.

When creating the msm object, the user must specify which covariates are time-varying ('time_varying covariates'). This code identifies all possible confounding variables and creates a simplified dataset for imputation. This is primarily a data-driven approach to identifying potential confounds. However, the user has the option to specify any covariates to be sure to exclude ('exclude_covariates') as well as those to include ('keep_covariates'), for theoretical or other reasons. The user can include time-invariant or time-varying covariates in these fields. Append ".x" (e.g., "INR.6) to the covariate to indicate a time-varying variable at a specific time point only, otherwise the condition will be assumed to all time points of any time-varying variables. 

*note from IS: not sure how to suppress the html code from console (i.e., to only save it out and not print when running)
```{r}

#identifies the total number covariates available in the dataset and prints them for user inspection
potential_covariates <- identifyCovariates(object, data)

#of those covariates, identify those that correlate with each exposure or outcome at each time point at >0.1 and thus could be potential confounders
covariates_to_include <- identifyPotentialConfounds(object, time_pt_datasets)

#create final dataset for imputations with only the necessary variables
data_to_impute <- dataToImpute(object, covariates_to_include)

```


### 4. Impute dataset so that each time point has complete data
ONlY run this code when needed! Creates 'm' imputed datasets using the Amelia package (Honaker, King, & Blackwell, 2011) and functions to eventually generate weights for each imputed dataset (that will be combined at the end). This code takes a few minutes to run per imputation. 

The parameter 'read_imps_from_file' will allow you to read imputed data in from local storage (="yes") so as not to have to re-run this imputation code or impute data (="no"; default). The number of imputed datasets is specified as 'm' in the msm object. 
```{r}

#creates imputed datasets and saves them out
imputed_datasets <- imputeData(object, data_to_impute, read_imps_from_file="no", 
                             max.resample = 100, cs=NULL, priors=NULL, lags=NULL, intercs=FALSE, leads=NULL, splinetime=NULL, logs=NULL, 
                             sqrts=NULL, lgstc=NULL, noms=NULL, bounds=NULL)

```


### 5. Format each imputed dataset for calculating balancing weights 
Making sure imputed datasets have the appropriate variables and are formatted in the "wide/long" format needed to create the balancing weights. This code also saves out a dataset structured for future modeling. 

```{r}

#creates wide/long datasets for each imputation
wide_long_datasets <- formatForWeights(object, data, imputed_datasets)

```


### 6. Create formulas for calculating balancing weights for each exposure-outcome pair at each time point 
Balancing weights focus on the relationship between potential confounding variables and each exposure. The following code creates formulas relating all appropriate concurrent and lagged covariates to each exposure at each time point. These will be used to create balancing weights in subsequent steps. 

This step is crucial --the user is advised to thoroughly inspect each formula and consider excluding potential collider variables at the msm object stage. 
```{r}

#creates formulas for each exposure-outcome pairing at each time point and saves and prints them for user inspection
forms <- createForms(object, wide_long_datasets, covariates_to_include)

```


### 7. Create balancing weights for each exposure-outcome pair at each time point for each imputed dataset
This step creates balancing weights using the Covariate Balancing Propensity Score (CBPS) package and approach (Imai & Ratkovic, 2014; Fong, 2022). Please see the CBPS package documentation (https://cran.r-project.org/web/packages/CBPS/CBPS.pdf) for more information about the other optional user parameters.

This code takes a little while to run. The parameter, 'read_in_from_file', allows you to instead read in weights that have already been saved locally (="yes") or to create weights (="no"; default).
```{r}

#creates balancing weights or reads them in from local file
weights_models <- createWeights(object, wide_long_datasets, forms, read_in_from_file="no",
                             ATT=0, iterations=1000, standardize=FALSE, method="exact", twostep=TRUE, sample.weights=NULL, 
                             baseline.formula=NULL, diff.formula=NULL) #optional weights parameters (see CBPS documentation for more)

```


### 8. Assess balance and determine variables for which balance could not be achieved for use as covariates in final model
This code evaluates which covariates are still correlated with exposures (i.e., for which balancing was not successful) above the designated threshold which the user specified in the msm object, after averaging across imputed datasets, and saves out a list of those covariates to include in your final model. 

```{r}

#assesses covariate balance and produces figures for user inspection
unbalanced_covariates_for_models <- assessBalance(object, weights_models)

```


### 9. Condense weights to create one per person per exposure-outcome pair
This code multiplies weights across time points and averages across imputed datasets to condense down to final individual weights per exposure-outcome pair for modeling.
```{r}
#condenses weights across imputed dataset and time point 
data_for_model_with_weights <- condenseWeights(object, weights_models)

```


### 10. Truncate final weights to avoid heavy tails
Given that weights with heavy tails can be problematic, this code caps weight values at a designated percentile (that the user has the option to specify with 'percentile_cutoff' when creating the msm object), populating all weights larger than that value with that percentile value. These weights can now be used in the formal marginal structural model, or weighted model. 

You can stop here if you have specific MSM models you want to fit or you can continue on to fit a standard single-outcome model. We also suggest conducting sensitivity/robustness tests by creating weights using several percentile cutoff values and ensuring your subsequent models produce similar results regardless of which percentile cutoff you used. 
```{r}
#truncate weights 
data_for_model_with_weights_cutoff <- truncateWeights(object, data_for_model_with_weights)

```


Please choose either steps 11-13 if you have a single value outcome at a single time point or step 14 if you have a growth process for your outcome at a single time point. 


### 11. Fit a series of marginal structural models to determine best fit
This code fits a series of weighted generalized linear models using the 'survey' package (Lumley, 2021) using the final truncated weights to separately examine the putative causal effects of each exposure on each (single time point) outcome, testing for the need for any covariates that were not successfully balanced. Fit is assessed using AIC. 

The current approach retains main effects of each exposure epoch for the purpose of examining different histories in the next step, and tests for all 2-way interactions between exposure main effects. 
Model dictionary:
*m0 baseline model with only main effects for each exposure epoch
*m1 full covariate model adding in all unbalanced covariates (if any)
*m2 covariate model including only significant covariates (retaining all epoch main effects)
*m3 full interaction model adding in all possible epoch main effect interactions
*m4 final model including all epoch main effects and only the significant covariates and interactions 

```{r}

#code to fit a series of baseline, covariates, and interaction weighted models to determine which is best fitting
all_models <- fitModel(object, data_for_model_with_weights_cutoff, unbalanced_covariates_for_models)

#determine best-fitting model  --ignore orange warnings here, could suppress them?
best_models <- assessModel(object, all_models)

```


### 12. Compare effects of exposure histories on outcomes and correct for multiple comparisons
This code fits a series of linear hypothesis tests using the 'car' package (Fox & Weisberg, 2019) to examine the effects of different combinations of exposures, or histories, on each outcome using the best-fitting model from the previous step. 

Because we use a continuous approach to modeling exposures, the user has the option to specify cutoffs for values of the exposures that will be considered high ("hi_cutoff") and low ("lo_cutoff") in the msm object. The user also has the option to specify a reference history ("reference") and an optional comparison history ("comparisons") on which to base comparisons, and which multiple comparison correction method ('method') to implement. 
```{r}

#conducts linear hypothesis testing using the best-fitting model for each exposure-outcome pairing
history_comparisons <- compareHistories(object, best_models)

#adjusts p-values for multiple comparisons with an optional user-specified method with Benjamini-Hochburg as the default
signifcant_comparisons <- mcCorrection(object, history_comparisons) #maybe try to suppress output?

```


### 13. Plot results by exposure history and dose
This code uses predicted values to plot the effects of the different exposure histories and resulting comparisons for each exposure-outcome pairing. Use the results from the above step to determine which exposure histories are significantly different from one another. Plots display outcome on the x-axis and exposure history on the y-axis, colored by dose (0-n(length of epochs)).

The user has the option to provide more formal labels for the exposure(s) ('exposure_labels') and outcome ('outcome_labels') in the msm object to be used for plotting.  
```{r}
#saves out plots of exposure histories for each out exposure-outcome pairing, colored by dosage of exposure 
plotResults(object, best_models)

```



### 14. Create an MPlus template for fitting weighted, more complex growth models for your single time point outcome
This code creates a template for fitting weighted more complex growth models in MPlus in the form of a .inp file. The code defaults to estimating slope (s) and quadratic (qd) growth and estimates contrasts examining the differences between different exposure histories. 

The user is advised to independently apply the appropriate transformations to their outcome measure and establish its appropriate functional form. They can then provide a .csv file to this function to create a template of an MPlus .inp file for fitting a growth model. The user can then modify the template .inp file as needed prior to running. 

[Meriah to add more info about what these models do]

assumes same missing indicator as original data
abridged dataset --only include necessary variables?
does up to 3-way interactions only
data must have outcomes in long form but exposures in wide form --check to see if there's a wide/long version of data saved out somewhere to guide the user?

Note for Meriah: any other variables/input fields that the user should specify? anything else to save out along the way?

Data should be modeled after the csv: TSST_allTxseq_LONG3.csv (which i manually put in the 'for Mplus' folder) which the user should create independently

The user can also view unbalanced covariates by time and averaged across time in the 'balance/post-balance correlation values/' folder. 
```{r}
#inspect the following exposure levels denoting all possible histories of high ("h") and low ("l") exposure over the epochs
# apply(gtools::permutations(2, nrow(object$exposure_epochs), c("L", "H"), repeats.allowed=TRUE), 1, paste, sep="", collapse="")


mplusObject <- mplusObject(
  #location of your data file in long format including final truncated weights (labeed exposure-outcome)
  data_file=paste0(object$home_dir, "for Mplus/TSST_allTxseq_LONG3.csv"),
  
  #list your outcomes and (optionally) any corresponding covariates you want included in each respective model
  additional_covariates=data.frame(outcomes=c("lnCORTR", "sqrtSAAR"), #names of outcomes that are in long form (MLD)
                        additional_covariates=I(list(c("Timeofday_c"), c("SubStressR_mn","Timeofday_c")))) #like methods covariates that don't need balancing
)


#create .imp files and header-less .csv files for each exposure-outcome pairing. This code adapts content from Gottfredson et al., 2019 via the aMNLFA package. 
makeMplusInputs(object, mplusObject)


#automatically run all inputs in the Mplus folder after you have manually modified/checked them
MplusAutomation::runModels(replaceOutfile = "never",  target = paste0(object$home_dir, 'for MPlus/'))

```


References --add Gottfredson, Stuart, Imai, etc. 
```{r}
print(citation("survey"))
print(citation("CBPS"))
print(citation("Amelia"))
print(citation("car"))

```

