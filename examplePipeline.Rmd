---
title: "examplePipeline"
author: "Isa Stallworthy"
date: "8/26/2022"
output: html_document
---

[insert brief intro to MSMs here and why they are important for developmental science]


Example pipeline running the MSM functions in sequence based on analyses using the Family Life Project (FLP).

Steps marked with * indicate an optional step if you have already run that code in the past (and thus have the outputs saved locally). 

## User inputs
```{r}
#directories to long dataset with all exposures, outcomes, and covariates in long format and project directory
data_path="/Users/isabella/Desktop/BSL Lab/FLP/New/data_long.csv"
home_dir= "/Users/isabella/Desktop/BSL Lab/FLP/New/"

#individual identifier 
ID="s_id"

#time points of interest and name of time variable in long dataset
time_pts=c(6, 15, 24, 35, 58)
time_var="TAge"

#marker for missing data
missing=-9999

#number of imputations
m=5

#exposures of interest that potentially have causal effects on outcomes and exposure time point(s) (corresponding to values in time_var)
exposures=c("HOMEETA1", "INR", "ESETA1", "CTSETA1")
# exposures=c("HOMEETA1", "INR")
exposure_time_pts=c(6, 15, 25, 36)

#developmental time periods and their corresponding time points that constitute regime/history units --these should be meaningful periods of time that may collapse over exposure time points to result in more manageable combinations for exploring effects exposure histories (e.g, chronically high exposure vs. exposure only in infancy)
exposure_epochs=data.frame(epochs=c("early", "middle", "late"), #names of epochs 
                           values=I(list(c(6,15), c(24,35), c(58)))) #time point(s) encompassed in each epoch that correspond to data

#outcomes of interest and outcome time point(s)
# outcomes=c("pcx_sensitive", "pcx_pomd", "pcx_nomd", "pcx_qrel", "pcx_CompTwo")
outcomes=c("pcx_sensitive", "pcx_pomd")
outcome_time_pts=58

```


### 1. Define your causal question 
Prints the causal questions you wish to answer using MSMs
```{r}

defineCausalQuestion(exposures, exposure_time_pts, outcomes, outcome_time_pts)

```


### 2. Format data & identify potential covariate confounding variables within the data provided
These functions requires a clean dataset in long format that contains columns for ID, time, exposure, outcome, and all potential covariate confounds as columns.
```{r}
#list any covariates that are factors here 
factor_covariates=c("NC","TcSex2", "TcBlac2") 

#Creates the necessary directories in your home directory and formats your dataset
data=formatDataStruct(data_path, home_dir, missing, factor_covariates)

#Makes separate datasets for each time point 
time_pt_datasets= makeTimePtDatasets(data, ID, time_pts)

```


### 3. Identify which covariates could potentially confound the relationship between exposure and outcome that will be used to create balancing weights
MSMs focus on the covariates that are associated with both exposure and outcome and thus could act like a confounding variable. This code assumes that you have included all possible confounding variables as columns in your dataset. 
```{r}
#list out any variables that should be excluded from balancing based on practical or theoretical reasons; default set to none
exclude_covariates=c("Index1")

#identifies the total number covariates available in the dataset and prints them for user inspection
potential_covariates=identifyCovariates(data, ID, exposures, outcomes, exclude_covariates)

#list out covariates that are time-varying
time_varying_covariates= c("INR", "WIND", "pcx_sensitive", "BSI","ESETA1", "HOMEETA1", "CTSETA1", "pcx_pos", "pcx_neg", "pcx_pomd","pcx_nomd", "pcx_qrel", "GrosPay1", "pcx_engaged", "pcx_CompTwo")

#of those covariates, identify those that correlate with each exposure or outcome at each time point at >0.1 and thus could be potential confounders
covariates_to_include= identifyPotentialConfounds(home_dir, data, time_pt_datasets, time_pts, exclude_covariates=NULL)

#create final dataset for imputations with only the necessary variables
data_to_impute=dataToImpute(ID, data, exposures, outcomes, covariates_to_include, exclude_covariates)

```


### 4. Impute dataset so that each time point has complete data*
Create m imputed datasets using the Amelia package and functions and weights will be generated for each imputed dataset (and combined at the end). ONly run this when needed! This code takes a few minutes to run per imputation. The next step of code, parameter 'just_imputed', will allow you to read imputed data in from local storage so as not to have to re-run. 
```{r}

#list continous variables here; all others assumed ordinal for imputation 
continuous_variables=c("GrosPay1", "SwghtLB", "RMomAgeU", "RMAge1st", "RWghtLb", "IBQDnovm", "MDI", "INR", "WIND", "ECBQ", "HOMEETA1", "CTSETA1", "ESETA1", "pcx_sensitive", "pcx_pos", "pcx_neg", "pcx_engaged", "pcx_sensitive", "SHCircCm", "pcx_CompTwo", "pcx_pomd", "pcx_nomd", "pcx_qrel", "BSI", "AGE")

imputed_datasets= imputeData(home_dir, ID, data_to_impute, continuous_variables, m, max.resample = 100, cs=NULL, priors=NULL, lags=NULL, intercs=FALSE, leads=NULL, splinetime=NULL, logs=NULL, sqrts=NULL, lgstc=NULL, noms=NULL, bounds=NULL)

```


### 5. Format each imputed dataset for calculating balancing weights 
Making sure imputed datasets have the appropriate variables and are formatted in "wide/long" format to create the balancing weights. This code also saves out a dataset structured for future modeling. The parameter 'just_imputed' allows you to specify whether you just ran the imputed code ("yes") or you'd rather read in the imputations saved locally from a previous run ("no"). 
```{r}
#list any time-varying variables that should not be present in the datasets because of planned missingness design 
time_var_exclude=c("WIND.1", "WIND.24", "BSI.35", "pcx_CompTwo.6", "pcx_CompTwo.1", "pcx_CompTwo.2", "pcx_CompTwo.35")

#creates wide/long datasets for each imputation. if you have just created imputations in the global environment, select just_imputed="yes", otherwise "no" will indicate to read them in from local storage
wide_long_datasets=formatForWeights(ID, home_dir, m, data, imputed_datasets, time_varying_covariates, time_pts, time_var_exclude, just_imputed="no")

```


### 6. Create formulae for calculating balancing weights, for each exposure at each time point 
Balancing weights focus on the relationship between the covariates that could be potential confounding variables and each exposure. The following code creates formulae relating all appropriate concurrent and lagged covariates to each exposure at each time point. These will be used to create balancing weight. This step is crucial --the user is advised to thoroughly inspect each formula and consider excluding potential collider variables. 
```{r}
#when creating a balancing formula at each time point, we need to remove potential colliders at that time point. list potential colliders (i.e., variables that could cause both the treatment and the outcome) at each time point below. these will be EXCLUDED from balancing only at the time point of the exposure as balancing on colliders can lead to problematic outcomes. the code automatically removes other outcomes. 
potential_colliders=c()

#creates formulae for each exposure at each time point and saves and prints them for user inspection
forms=createForms(wide_long_datasets,covariates_to_include, exposures, outcomes, time_pts, potential_colliders)

```


### 7. Creating balancing weights for each exposure at each time point, for each imputed dataset*
This step creates balancing weights using the CBPS package and function. Only run this code when needed! This code takes a little while to run and the parameter 'just_made_weights' in the next step allows you to instead read in weights that have already been saved locally. 
```{r}

weights_models=createWeights(wide_long_datasets,forms, exposures, time_pts, m,  ATT=0, iterations=1000, standardize=FALSE, method="exact", twostep=TRUE, sample.weights=NULL, baseline.forumula=NULL, diff.formula=NULL)


```


### 8. Assess balance and determine variables for which balance could not be achieved for use as covariates in final model
This code evaluates which covariates are still correlated with exposures above the designated threshold (i.e., for which balancing was not successful) which the user has the option to define ('balance_thresh'), after averaging across imputed datasets and time points, and saves out a list of those covariates to include in your final model. Use the parameter 'just_made_weights' to designate whether you just made the weights and they are in the global environment ('yes') or you'd rather read them in from an earlier run ('no'). 
```{r}
#assesses covariate balance and produces figures for user inspection
unbalanced_covariates_for_models=assessBalance(home_dir, weights_models=list(), m, exposures, time_pts, balance_thresh=0.12, just_made_weights="no")

#reads in weights models to global environment
weights_models=getWeights(home_dir, just_made_weights="no")

```


### 9. Condense weights to create one per person per exposure
This code multiplies weights across time points and averages across imputed datasets to condense down to final individual weights per exposure for modeling.
```{r}

data_for_model_with_weights=condenseWeights(ID, home_dir, m, weights_models, exposures, time_pts)


```


### 10. Truncate final weights to avoid heavy tails
This code caps weights at a designated percentile, populating all weights larger than that value with that percentile value. These weights can now be used in the formal marginal structural model, or weighted model. You can stop here if you have specific MSM models you want to fit or you can continue on to fit a single-outcome model. 
```{r}

data_for_model_with_weights_cutoff=truncateWeights(data_for_model_with_weights, home_dir, exposures, percentile_cutoff=0.90)

```


### 11. Fit a series of marginal structural models to determine best fit
This code fits a series of weighted generalized linear models (using the 'survey' package) using the final truncated weights to separately examine the putative causal effects of each of your exposures on each (single time point) outcome, including any covariates that were not successfully balanced. Fit is assessed using AIC. The current approach retains main effects of each exposure epoch for the purpose of examining different histories in the next step, and tests for all 2-way interactions between exposure main effects. 
Model dictionary:
  *m0 baseline model with only main effects for each exposure epoch
  *m1 full covariate model adding in all unbalanced covariates (if any)
  *m2 covariate model including only significant covariates (retaining all epoch main effects)
  *m3 full interaction model adding in all possible epoch main effect interactions
  *m4 final model including all epoch main effects and only the significant covariates and interactions 
```{r}

#code to fit a series of baseline, covariates, and interaction weighted models to determine which is best fitting
all_models=fitModel(home_dir, ID, exposures, exposure_epochs, outcomes, outcome_time_pts, data_for_model_with_weights_cutoff, unbalanced_covariates_for_models)

#determine best fitting model
best_models=assessModel(home_dir, all_models)


```


### 12. Compare exposure histories and correct for multiple comparisons
This code fits a series of linear hypothesis tests to examine the effects of different combinations of exposures, or histories/regimes) on each outcome using the best-fitting model from the previous step. Because we use a continuous approach, the user has the option to specify cutoffs for values of the exposures that will be considered high ("hi_cutoff") and low ("lo_cutoff"). The user also has the option to specify a reference history ("reference") and an optional comparison history ("comparisons") on which to base comparisons and which multiple comparison correction method ('method') to implement. 
```{r}
#inspect the following exposure levels denoting all possible histories of high ("h") and low ("l") exposure over the epochs
apply(gtools::permutations(2, nrow(exposure_epochs), c("l", "h"), repeats.allowed=TRUE), 1, paste, sep="", collapse="-")

#select one of the above histories as your reference event, or the history/sequence to which you would like to compare the other histories  
reference="l-l-l" #optional: set a reference history from the list above; the default is set to the history denoting low exposure at all time points 
comparisons="" #optional: set a comparison history from list above; the default is to include all non-reference histories as comparisons

#conducts linear hypothesis testing using the best-fitting model for each exposure-outcome pairing
history_comparisons=compareHistories(home_dir, exposures, exposure_epochs, hi_cutoff=.75, lo_cutoff=.25, outcome, outcome_time_pts, best_models, comparisons, reference)

#adjust p-values for multiple comparisons with an optional user-specified method with Benjamini-Hochburg as the default
signifcant_comparisons=mcCorrection(history_comparisons, method="BH")

```


### 13. Plot results by exposure history and dose
This code uses predicted values to plot the effects of the different exposure histories and resulting comparisons for each exposure-outcome pairing. Use the results from the above step to determine which exposure histories are significantly different from one another. The user has the option to provide more formal labels for the exposure(s) ('exposure_labels') and outcome ('outcome_labels') to be used for plotting.  
```{r}
#saves out plots of exposure histories for each out exposure-outcome pairing, colored by dosage of exposure 
plotResults(home_dir, exposures, outcomes, best_models, exposure_labels=c("Home Resources", "Income-to-Needs-Ratio", "Threat", "Conflict"), outcome_labels=c("Sensitive Parenting","Parent Positive Affect"))

```

